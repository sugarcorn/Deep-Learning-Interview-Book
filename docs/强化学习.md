[TOC]

# 强化学习
![Logistic Regression.png](https://github.com/vivienzou1/Deep-Learning-Interview-Book/blob/master/docs/imgs/DRL26.png)

Policy 的方法，其优缺点总结如下：

　　Advantages:
　　　　1. Better convergence properties （更好的收敛属性）
　　　　2. Effective in high-dimensional or continuous action spaces（在高维度和连续动作空间更加有效）
　　　　3. Can learn stochastic policies（可以Stochastic 的策略）
　　Disadvantages:
　　　　1. Typically converge to a local rather than global optimum（通常得到的都是局部最优解）
　　　　2. Evaluating a policy is typically inefficient and high variance （评价策略通常不是非常高效，并且有很高的偏差）
    
 ![Logistic Regression.png](https://github.com/vivienzou1/Deep-Learning-Interview-Book/blob/master/docs/imgs/DRL31.png)
    
## 强化学习解决的是什么样的问题？

对应当前的动作会影响环境的状态，也即下一个状态会和上一个状态与采取的动作相关，服从马尔科夫性.
对应模型未知，需要通过学习逐渐的逼近真实的模型.
模型未给定，因此需要和环境交互来学习，动作影响环境状态因此需要连续的决策来最大化累计回报.

## 举出强化学习与有监督学习的异同点。有监督学习靠样本标签训练模型，强化学习靠的是什么？
![Logistic Regression.png](https://github.com/vivienzou1/Deep-Learning-Interview-Book/blob/master/docs/imgs/DRL10.png)
![Logistic Regression.png](https://github.com/vivienzou1/Deep-Learning-Interview-Book/blob/master/docs/imgs/DRL11.png)

1. 有监督学习的训练样本是有标签的，强化学习的训练是没有标签的，它是通过环境给出的奖惩来学习
2. 有监督学习的学习过程是静态的，强化学习的学习过程是动态的。这里静态与动态的区别在于是否会与环境进行交互，有监督学习是给什么样本就学什么，而强化学习是要和环境进行交互，再通过环境给出的奖惩来学习
3. 有监督学习解决的更多是感知问题，尤其是深度学习，强化学习解决的主要是决策问题。因此有监督学习更像是五官，而强化学习更像大脑。

## 强化学习的损失函数（loss function）是什么？

The loss function here is mean squared error of the predicted Q-value and the target Q-value – Q*. This is basically a regression problem. Going back to the Q-value update equation derived fromthe Bellman equation. we have:

![Logistic Regression.png](https://github.com/vivienzou1/Deep-Learning-Interview-Book/blob/master/docs/imgs/DRL12.png)

The section in green represents the target. it is predicting its own value, but since R is the unbiased true reward, the network is going to update its gradient using backpropagation to finally converge.

## 写贝尔曼方程（Bellman Equation）

![Logistic Regression.png](https://github.com/vivienzou1/Deep-Learning-Interview-Book/blob/master/docs/imgs/DRL13.png)
given by action time s, the reward + gamma * value of action in time s+1

**参考资料**

- [贝尔曼方程](https://blog.csdn.net/zbgzzz/article/details/80962645)

## 最优值函数和最优策略为什么等价？

强化学习算法可以分为三大类：value based, policy based 和 actor critic

value based: the policy is based on the max value of each state stored in the value function

## 求解Monte Carlo决策过程都有哪些方法？

* Components of an MDP:
1. Agent
2. Environment
3. State
4. Action
5. Reward

trajectory: shows the sequence of states, actions, and rewards.
agent goal: to maximize the cumulative rewards.

![Logistic Regression.png](https://github.com/vivienzou1/Deep-Learning-Interview-Book/blob/master/docs/imgs/DRL14.png)
![Logistic Regression.png](https://github.com/vivienzou1/Deep-Learning-Interview-Book/blob/master/docs/imgs/DRL15.png)

## 简述蒙特卡罗估计值函数的算法。

![Logistic Regression.png](https://github.com/vivienzou1/Deep-Learning-Interview-Book/blob/master/docs/imgs/DRL20.png)
- 思路：通过经验（状态、动作和奖励的样本序列）求解最优策略。比如在初始状态s，遵循策略π，最终获得了总回报R，这就是一个样本。如果我们有许多这样的样本，就可以估计在状态s下，遵循策略π的期望回报。
- 优势： 蒙特卡罗方法不需要对环境的完整知识（区别于DP）。仅仅需要经验（状态、动作和奖励的样本序列）就可以求解最优策略，这些经验可以在线获得或者根据某种模拟机制获得。
- 缺点：蒙特卡罗方法必须等到episode task的末尾才能确定到V(St)的增量(这时Gt才知道)，只能用于episode tasks。


## 简述temporal-difference learning 算法
a way to incrementally estimate the return through boostrape. 
直接从经验中学习而不需要对于环境的完整知识。类似动态规划方法，它能够在现有的估计结果上进行提升而不需要等待整个事件结束。

![Logistic Regression.png](https://github.com/vivienzou1/Deep-Learning-Interview-Book/blob/master/docs/imgs/DRL21.png)
- 优势： TD方法比DP方法有一个优点，即它们不需要环境模型、回报模型和下一状态概率分布模型。TD方法比蒙特卡罗方法的一个最明显的优点是，它们自然地以在线、完全增量的方式实现，不局限于episode task，可以用于连续的任务。
- 时间差分学习会进行极大似然估计，使得估计的结果更有可能符合未来数据的趋势。
- 但是在收敛速度上，TD方法与MC方法哪个好并没有结论。
- 缺点： 大规模MDP或连续空间MDP，TD是很难保证在每一个时间步长更新，因此，很难保证及时性。
![Logistic Regression.png](https://github.com/vivienzou1/Deep-Learning-Interview-Book/blob/master/docs/imgs/DRL19.png)

## 介绍Q-Learning

off-policy, model-free RL algorithm based on the well-known Bellman Equation:
![Logistic Regression.png](https://github.com/vivienzou1/Deep-Learning-Interview-Book/blob/master/docs/imgs/DRL22.png)

E in the above equation refers to the expectation, while ƛ refers to the discount factor. 

**参考资料**

- [Q-Learning](http://mnemstudio.org/path-finding-q-learning-tutorial.htm)

- [Q-learning算法](https://www.jianshu.com/p/eecb2230decf)
- [【强化学习】Q-Learning算法详解](https://blog.csdn.net/qq_30615903/article/details/80739243)

- [通过 Q-learning 深入理解强化学习](https://www.jiqizhixin.com/articles/2018-04-17-3)

## DQN 算法
强化学习算法可以分为三大类：value based, policy based 和 actor critic
DQN - value based, only has value network and do not have policy network

一个state action pair (s,a)对应一个值函数Q(s,a),理论上对于任意的(s,a)我们都可以由公式求出它的值函数，即用一个查询表lookup table来表示值函数。

### 基本原理

DQN使用神经网络来近似值函数，即神经网络的输入是state s,输出是Q(s,a),∀ a ∈A (action space)。通过神经网络计算出值函数后，DQN使用ϵ−greedy策略来输出action（第四部分中介绍）。值函数网络与ϵ−greedy策略之间的联系是这样的：首先环境会给出一个obs，智能体根据值函数网络得到关于这个obs的所有Q(s,a)，然后用ϵ−greedy选择action并做出决策，环境接收到此action后会给出一个奖励Rew及下一个obs。

但是在普通的Q-learning中，当状态和动作空间是离散且维数不高时可使用Q-Table储存每个状态动作对的Q值，而当状态和动作空间是高维连续时，使用Q-Table不动作空间和状态太大十分困难。
所以在此处可以把Q-table更新转化为一approximate function合问题，通过拟合一个approximate function来代替Q-table产生Q值，使得相近的状态得到相近的输出动作。
![Logistic Regression.png](https://github.com/vivienzou1/Deep-Learning-Interview-Book/blob/master/docs/imgs/DRL3.png)
![Logistic Regression.png](https://github.com/vivienzou1/Deep-Learning-Interview-Book/blob/master/docs/imgs/DRL4.png)

* DL与RL结合存在以下问题
1. DL是监督学习需要学习训练集，强化学习不需要训练集只通过环境进行返回奖励值reward，同时也存在着噪声和延迟的问题，所以存在很多状态state的reward值都是0也就是样本稀疏
2. DL每个样本之间互相独立，而RL当前状态的状态值是依赖后面的状态返回值的。
3. 当我们使用非线性网络来表示值函数的时候可能出现不稳定的问题

* DQN中的两大利器解决了以上问题
1. 通过Q-Learning使用reward来构造标签
2. 通过experience replay（经验池）的方法来解决相关性及非静态分布问题
3. 使用一个MainNet产生当前Q值，使用另外一个Target产生Target Q

**参考资料**

- [【强化学习】Deep Q Network(DQN)算法详解](https://blog.csdn.net/qq_30615903/article/details/80744083)

### DQN的两个关键trick分别是什么？
1. experience replay
一种 off-policy 离线学习法.
它能学习当前经历着的, 过去经历过的, 随机的加入之前的经验会让神经网络更有效率。
他通过在每个timestep下agent与环境交互得到的转移样本(S_t, A_t, R_t, S_t+1)储存到回放记忆网络，要训练时就随机拿出一些（minibatch）来训练因此打乱其中的相关性。
**randomlly feed the minibatch with learnt step against steps from experience replay**
![Logistic Regression.png](https://github.com/vivienzou1/Deep-Learning-Interview-Book/blob/master/docs/imgs/DRL5.png)
![Logistic Regression.png](https://github.com/vivienzou1/Deep-Learning-Interview-Book/blob/master/docs/imgs/DRL6.png)

2. Target network
Estimated Network和Target Network不能同时更新参数，应该另设Target Network以保证稳定性)
* Target network
the agent make prediction, what we predict against each step
![Logistic Regression.png](https://github.com/vivienzou1/Deep-Learning-Interview-Book/blob/master/docs/imgs/DRL1.png)

* main network
get trained very step
![Logistic Regression.png](https://github.com/vivienzou1/Deep-Learning-Interview-Book/blob/master/docs/imgs/DRL2.png)

### DQN 都有哪些变种？DQN有哪些改进方向？

##### Double-DQN：将动作选择和价值估计分开，避免价值过高估计。
为了解决原始DQN存在的过估计问题。过估计是指估计的值函数比真实的值函数大。

为了解决过估计的问题，Hasselt提出了Double Q Learning方法，将此方法应用到DQN中，就是Double DQN，即DDQN。所谓的Double Q Learning是将动作的选择和动作的评估分别用不同的值函数来实现。动作的选择：选择该状态下动作值函数最大的动作，即 a* 。动作的选择所用的动作值函数:
`argmax_a Q(S_t+1, a; theta_t)`
动作的评估：选出 a* 后，利用 a* 处的动作值函数构造TD目标，TD目标公式为：
![Logistic Regression.png](https://github.com/vivienzou1/Deep-Learning-Interview-Book/blob/master/docs/imgs/DRL7.png)

##### Dueling-DQN：将Q值分解为状态价值和优势函数，得到更多有用信息。
adding state reward
![Logistic Regression.png](https://github.com/vivienzou1/Deep-Learning-Interview-Book/blob/master/docs/imgs/DRL9.png)

##### Prioritized Replay Buffer：将经验池中的经验按照优先级进行采样。
如果每次抽样都需要针对 p 对所有样本排序, 这将会是一件非常消耗计算能力的事. 可以采用更高级的算法——SumTree方法。
SumTree是一种树形结构，每片树叶存储每个样本的优先级P，每个树枝节点只有两个分叉，节点的值是两个分叉的和，所以SumTree的顶端就是所有p的和。

简而言之，每次我们都选较大的节点数，因为每个父节点都是两个子节点的和，那么父节点较大的数也对应着较大的子节点数。
![Logistic Regression.png](https://github.com/vivienzou1/Deep-Learning-Interview-Book/blob/master/docs/imgs/DRL8.png)

### 引入状态奖励的是哪种DQN？

dueling DQN

### Dueling DQN和DQN有什么区别？

Dueling DQN has two streams: state-value and the advantages for each action

## 介绍OpenAI用的PPO算法

- [ ] TODO

## 介绍TRPO算法

- [ ] TODO

## 为什么TRPO能保证新策略的回报函数单调不减？

- [ ] TODO

## 介绍DDPG算法
agent需要的主要学习内容：第一是行为策略(action policy)， 第二是规划(planning)。

行为(action)可以简单分为：
- 连续的：如赛车游戏中的方向盘角度、油门、刹车控制信号，机器人的关节伺服电机控制信号。
- 离散的：如围棋、贪吃蛇游戏。 Alpha Go就是一个典型的离散行为agent。

DDPG是针对连续行为的策略学习方法。

γ 叫做discounted rate, ∈[0,1] ,通常取0.99.

Deep Deterministic Policy Gradient,是将深度学习神经网络融合进DPG的策略学习方法。

**相对于DPG的核心改进:**
1. 使用卷积神经网络来模拟策略函数和Q函数，并用深度学习的方法来训练，证明了在RL方法中，非线性模拟函数的准确性和高性能、可收敛；
- 而DPG中，可以看成使用线性回归的机器学习方法：使用带参数的线性函数来模拟策略函数和Q函数，然后使用线性回归的方法进行训练。

2. experience replay memory的使用：actor同环境交互时，产生的transition数据序列是在时间上高度关联(correlated)的，如果这些数据序列直接用于训练，会导致神经网络的overfit，不易收敛。
- DDPG的actor将transition数据先存入experience replay buffer, 然后在训练时，从experience replay buffer中随机采样mini-batch数据，这样采样得到的数据可以认为是无关联的。

3. target 网络和online 网络的使用， 使的学习过程更加稳定，收敛更有保障。

## 画出DDPG框架
![Logistic Regression.png](https://github.com/vivienzou1/Deep-Learning-Interview-Book/blob/master/docs/imgs/DRL25.png)

## DDPG中的第二个D 为什么要确定？

* PG
![Logistic Regression.png](https://github.com/vivienzou1/Deep-Learning-Interview-Book/blob/master/docs/imgs/DRL23.png)

* DPG
确定性的行为策略
![Logistic Regression.png](https://github.com/vivienzou1/Deep-Learning-Interview-Book/blob/master/docs/imgs/DRL24.png)

[Deep Reinforcement Learning - 1. DDPG原理和算法](https://blog.csdn.net/kenneth_yu/article/details/78478356)

## 介绍A3C算法

![Logistic Regression.png](https://github.com/vivienzou1/Deep-Learning-Interview-Book/blob/master/docs/imgs/DRL28.png)
![Logistic Regression.png](https://github.com/vivienzou1/Deep-Learning-Interview-Book/blob/master/docs/imgs/DRL29.png)
![Logistic Regression.png](https://github.com/vivienzou1/Deep-Learning-Interview-Book/blob/master/docs/imgs/DRL30.png)

**参考资料**

- [一文读懂 深度强化学习算法 A3C （Actor-Critic Algorithm）](https://www.cnblogs.com/wangxiaocvpr/p/8110120.html)
- [深度强化学习——A3C](https://blog.csdn.net/u013236946/article/details/73195035/)

## A3C中advantage value意义

advantage value的估计关键在于使用了与标准Q-网络一样的多歩奖励。尽管使用了不同时间歩长的奖励值，但我们仍能得到一个advantage value(我会说更好)的估计。在每个时间歩长中，单个线程累积梯度，并在到达“更新点”时，把梯度更新到核心的参数服务器。advantage value通常为0，且在游戏结束的时候应该恒为0。

## 强化学习如何用在推荐系统中？

![Logistic Regression.png](https://github.com/vivienzou1/Deep-Learning-Interview-Book/blob/master/docs/imgs/DRL32.png)

**参考资料**

- [ ] [用强化学习研究推荐系统的前景和难度怎么样？](https://www.zhihu.com/question/328133447)
- [ ] [深度强化学习如何和推荐系统结合起来？](https://www.zhihu.com/question/63037952)
- [ ] [ICML 2019 | 强化学习用于推荐系统，蚂蚁金服提出生成对抗用户模型](https://zhuanlan.zhihu.com/p/68029391)
- [ ] [最新！五大顶会2019必读的深度推荐系统与CTR预估相关的论文](https://zhuanlan.zhihu.com/p/69050253)

## 介绍Sarsa算法

![Logistic Regression.png](https://github.com/vivienzou1/Deep-Learning-Interview-Book/blob/master/docs/imgs/DRL34.png)

**参考资料**

- [AI学习笔记——Sarsa算法](https://www.jianshu.com/p/9bbe5aa3924b)

## Sarsa 和 Q-Learning区别

![Logistic Regression.png](https://github.com/vivienzou1/Deep-Learning-Interview-Book/blob/master/docs/imgs/DRL33.png)

当Sarsa 和 Q-Learning处在状态s时，均选择可带来最大回报的动作a，这样可到达状态s’。而在下一步，如果使用Q-Learning， 则会观察在s’上哪个动作会带来最大回报（不会真正执行该动作，仅用来更新Q表），在s’上做决定时, 再基于更新后的Q表选择动作。而 Sarsa 是实践派，在s’ 这一步估算的动作也是接下来要执行的动作，所以 Q(s, a) 的现实值也会稍稍改动，去掉maxQ，取而代之的是在s’ 上实实在在选取的a’ 的Q值，最后像Q-Learning一样求出现实和估计的差距并更新Q表里的Q(s, a)。

因为 Sarsa 是说到做到型，所以称它为 on-policy(在线学习)，即边做边学；而Q-Learning是说到但并不一定做到，其可以通过观察别人的经历来学习，所以它也叫作 Off-policy(离线学习)。

另外，Q-Learning因为有了 maxQ，所以也是一个特别勇敢的算法，原因在于它永远都会选择最近的一条通往成功的道路，不管这条路会有多危险。而 Sarsa 则是相当保守，它会选择离危险远远的，这就是使用Sarsa方法的不同之处。

**参考资料**

- [强化学习(五)：Sarsa算法与Q-Learning算法](https://blog.csdn.net/liweibin1994/article/details/79119056)
- [强化学习中的Q-learning算法和Sarsa算法的区别](https://blog.csdn.net/wshixinshouaaa/article/details/80832415)
- [Bourne强化学习笔记2：彻底搞清楚什么是Q-learning与Sarsa](https://blog.csdn.net/linyijiong/article/details/81607691)

## 强化学习 DQN，DDQN，AC，DDPG 的区别

- [ ] TODO

## 参考资料

- [再励学习面试真题](https://zhuanlan.zhihu.com/p/33133828)
- [强化学习面经](https://zhuanlan.zhihu.com/p/44285282)

## Hidden Markov
![Logistic Regression.png](https://github.com/vivienzou1/Deep-Learning-Interview-Book/blob/master/docs/imgs/DRL35.png)
[HMM](http://xtf615.com/2017/07/15/RL/)
