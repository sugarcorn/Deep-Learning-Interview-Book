[TOC]

# 强化学习

## 强化学习解决的是什么样的问题？

对应当前的动作会影响环境的状态，也即下一个状态会和上一个状态与采取的动作相关，服从马尔科夫性.
对应模型未知，需要通过学习逐渐的逼近真实的模型.
模型未给定，因此需要和环境交互来学习，动作影响环境状态因此需要连续的决策来最大化累计回报.

## 举出强化学习与有监督学习的异同点。有监督学习靠样本标签训练模型，强化学习靠的是什么？
![Logistic Regression.png](https://github.com/vivienzou1/Deep-Learning-Interview-Book/blob/master/docs/imgs/DRL10.png)
![Logistic Regression.png](https://github.com/vivienzou1/Deep-Learning-Interview-Book/blob/master/docs/imgs/DRL11.png)

1. 有监督学习的训练样本是有标签的，强化学习的训练是没有标签的，它是通过环境给出的奖惩来学习
2. 有监督学习的学习过程是静态的，强化学习的学习过程是动态的。这里静态与动态的区别在于是否会与环境进行交互，有监督学习是给什么样本就学什么，而强化学习是要和环境进行交互，再通过环境给出的奖惩来学习
3. 有监督学习解决的更多是感知问题，尤其是深度学习，强化学习解决的主要是决策问题。因此有监督学习更像是五官，而强化学习更像大脑。

## 强化学习的损失函数（loss function）是什么？

The loss function here is mean squared error of the predicted Q-value and the target Q-value – Q*. This is basically a regression problem. Going back to the Q-value update equation derived fromthe Bellman equation. we have:

![Logistic Regression.png](https://github.com/vivienzou1/Deep-Learning-Interview-Book/blob/master/docs/imgs/DRL12.png)

The section in green represents the target. it is predicting its own value, but since R is the unbiased true reward, the network is going to update its gradient using backpropagation to finally converge.

## 写贝尔曼方程（Bellman Equation）

![Logistic Regression.png](https://github.com/vivienzou1/Deep-Learning-Interview-Book/blob/master/docs/imgs/DRL13.png)
given by action time s, the reward + gamma * value of action in time s+1

**参考资料**

- [贝尔曼方程](https://blog.csdn.net/zbgzzz/article/details/80962645)

## 最优值函数和最优策略为什么等价？

强化学习算法可以分为三大类：value based, policy based 和 actor critic

value based: the policy is based on the max value of each state stored in the value function

## 求解Monte Carlo决策过程都有哪些方法？

* Components of an MDP:
1. Agent
2. Environment
3. State
4. Action
5. Reward

trajectory: shows the sequence of states, actions, and rewards.
agent goal: to maximize the cumulative rewards.

![Logistic Regression.png](https://github.com/vivienzou1/Deep-Learning-Interview-Book/blob/master/docs/imgs/DRL14.png)
![Logistic Regression.png](https://github.com/vivienzou1/Deep-Learning-Interview-Book/blob/master/docs/imgs/DRL15.png)

## 简述蒙特卡罗估计值函数的算法。

![Logistic Regression.png](https://github.com/vivienzou1/Deep-Learning-Interview-Book/blob/master/docs/imgs/DRL16.png)
![Logistic Regression.png](https://github.com/vivienzou1/Deep-Learning-Interview-Book/blob/master/docs/imgs/DRL17.png)
![Logistic Regression.png](https://github.com/vivienzou1/Deep-Learning-Interview-Book/blob/master/docs/imgs/DRL18.png)

## 简述时间差分算法

- [ ] TODO

## 介绍Q-Learning

- [ ] TODO

**参考资料**

- [Q-Learning](http://mnemstudio.org/path-finding-q-learning-tutorial.htm)

- [Q-learning算法](https://www.jianshu.com/p/eecb2230decf)
- [【强化学习】Q-Learning算法详解](https://blog.csdn.net/qq_30615903/article/details/80739243)

- [通过 Q-learning 深入理解强化学习](https://www.jiqizhixin.com/articles/2018-04-17-3)

## DQN 算法
强化学习算法可以分为三大类：value based, policy based 和 actor critic
DQN - value based, only has value network and do not have policy network

一个state action pair (s,a)对应一个值函数Q(s,a),理论上对于任意的(s,a)我们都可以由公式求出它的值函数，即用一个查询表lookup table来表示值函数。

### 基本原理

DQN使用神经网络来近似值函数，即神经网络的输入是state s,输出是Q(s,a),∀ a ∈A (action space)。通过神经网络计算出值函数后，DQN使用ϵ−greedy策略来输出action（第四部分中介绍）。值函数网络与ϵ−greedy策略之间的联系是这样的：首先环境会给出一个obs，智能体根据值函数网络得到关于这个obs的所有Q(s,a)，然后用ϵ−greedy选择action并做出决策，环境接收到此action后会给出一个奖励Rew及下一个obs。

但是在普通的Q-learning中，当状态和动作空间是离散且维数不高时可使用Q-Table储存每个状态动作对的Q值，而当状态和动作空间是高维连续时，使用Q-Table不动作空间和状态太大十分困难。
所以在此处可以把Q-table更新转化为一approximate function合问题，通过拟合一个approximate function来代替Q-table产生Q值，使得相近的状态得到相近的输出动作。
![Logistic Regression.png](https://github.com/vivienzou1/Deep-Learning-Interview-Book/blob/master/docs/imgs/DRL3.png)
![Logistic Regression.png](https://github.com/vivienzou1/Deep-Learning-Interview-Book/blob/master/docs/imgs/DRL4.png)

* DL与RL结合存在以下问题
1. DL是监督学习需要学习训练集，强化学习不需要训练集只通过环境进行返回奖励值reward，同时也存在着噪声和延迟的问题，所以存在很多状态state的reward值都是0也就是样本稀疏
2. DL每个样本之间互相独立，而RL当前状态的状态值是依赖后面的状态返回值的。
3. 当我们使用非线性网络来表示值函数的时候可能出现不稳定的问题

* DQN中的两大利器解决了以上问题
1. 通过Q-Learning使用reward来构造标签
2. 通过experience replay（经验池）的方法来解决相关性及非静态分布问题
3. 使用一个MainNet产生当前Q值，使用另外一个Target产生Target Q

**参考资料**

- [【强化学习】Deep Q Network(DQN)算法详解](https://blog.csdn.net/qq_30615903/article/details/80744083)

### DQN的两个关键trick分别是什么？
1. experience replay
一种 off-policy 离线学习法.
它能学习当前经历着的, 过去经历过的, 随机的加入之前的经验会让神经网络更有效率。
他通过在每个timestep下agent与环境交互得到的转移样本(S_t, A_t, R_t, S_t+1)储存到回放记忆网络，要训练时就随机拿出一些（minibatch）来训练因此打乱其中的相关性。
**randomlly feed the minibatch with learnt step against steps from experience replay**
![Logistic Regression.png](https://github.com/vivienzou1/Deep-Learning-Interview-Book/blob/master/docs/imgs/DRL5.png)
![Logistic Regression.png](https://github.com/vivienzou1/Deep-Learning-Interview-Book/blob/master/docs/imgs/DRL6.png)

2. Target network
Estimated Network和Target Network不能同时更新参数，应该另设Target Network以保证稳定性)
* Target network
the agent make prediction, what we predict against each step
![Logistic Regression.png](https://github.com/vivienzou1/Deep-Learning-Interview-Book/blob/master/docs/imgs/DRL1.png)

* main network
get trained very step
![Logistic Regression.png](https://github.com/vivienzou1/Deep-Learning-Interview-Book/blob/master/docs/imgs/DRL2.png)

### DQN 都有哪些变种？DQN有哪些改进方向？

##### Double-DQN：将动作选择和价值估计分开，避免价值过高估计。
为了解决原始DQN存在的过估计问题。过估计是指估计的值函数比真实的值函数大。

为了解决过估计的问题，Hasselt提出了Double Q Learning方法，将此方法应用到DQN中，就是Double DQN，即DDQN。所谓的Double Q Learning是将动作的选择和动作的评估分别用不同的值函数来实现。动作的选择：选择该状态下动作值函数最大的动作，即 a* 。动作的选择所用的动作值函数:
`argmax_a Q(S_t+1, a; theta_t)`
动作的评估：选出 a* 后，利用 a* 处的动作值函数构造TD目标，TD目标公式为：
![Logistic Regression.png](https://github.com/vivienzou1/Deep-Learning-Interview-Book/blob/master/docs/imgs/DRL7.png)

##### Dueling-DQN：将Q值分解为状态价值和优势函数，得到更多有用信息。
adding state reward
![Logistic Regression.png](https://github.com/vivienzou1/Deep-Learning-Interview-Book/blob/master/docs/imgs/DRL9.png)

##### Prioritized Replay Buffer：将经验池中的经验按照优先级进行采样。
如果每次抽样都需要针对 p 对所有样本排序, 这将会是一件非常消耗计算能力的事. 可以采用更高级的算法——SumTree方法。
SumTree是一种树形结构，每片树叶存储每个样本的优先级P，每个树枝节点只有两个分叉，节点的值是两个分叉的和，所以SumTree的顶端就是所有p的和。

简而言之，每次我们都选较大的节点数，因为每个父节点都是两个子节点的和，那么父节点较大的数也对应着较大的子节点数。
![Logistic Regression.png](https://github.com/vivienzou1/Deep-Learning-Interview-Book/blob/master/docs/imgs/DRL8.png)

### 引入状态奖励的是哪种DQN？

dueling DQN

### Dueling DQN和DQN有什么区别？

Dueling DQN has two streams: state-value and the advantages for each action

## 介绍OpenAI用的PPO算法

- [ ] TODO

## 介绍TRPO算法

- [ ] TODO

## 为什么TRPO能保证新策略的回报函数单调不减？

- [ ] TODO

## 介绍DDPG算法

## 画出DDPG框架

## DDPG中的第二个D 为什么要确定？

- [ ] TODO

## 介绍A3C算法

- [ ] TODO

**参考资料**

- [一文读懂 深度强化学习算法 A3C （Actor-Critic Algorithm）](https://www.cnblogs.com/wangxiaocvpr/p/8110120.html)
- [深度强化学习——A3C](https://blog.csdn.net/u013236946/article/details/73195035/)

## A3C中优势函数意义

- [ ] TODO

## 强化学习如何用在推荐系统中？

- [ ] TODO

**参考资料**

- [ ] [用强化学习研究推荐系统的前景和难度怎么样？](https://www.zhihu.com/question/328133447)
- [ ] [深度强化学习如何和推荐系统结合起来？](https://www.zhihu.com/question/63037952)
- [ ] [ICML 2019 | 强化学习用于推荐系统，蚂蚁金服提出生成对抗用户模型](https://zhuanlan.zhihu.com/p/68029391)
- [ ] [最新！五大顶会2019必读的深度推荐系统与CTR预估相关的论文](https://zhuanlan.zhihu.com/p/69050253)

## 介绍Sarsa算法

- [ ] TODO

**参考资料**

- [AI学习笔记——Sarsa算法](https://www.jianshu.com/p/9bbe5aa3924b)

## Sarsa 和 Q-Learning区别

- [ ] TODO

**参考资料**

- [强化学习(五)：Sarsa算法与Q-Learning算法](https://blog.csdn.net/liweibin1994/article/details/79119056)
- [强化学习中的Q-learning算法和Sarsa算法的区别](https://blog.csdn.net/wshixinshouaaa/article/details/80832415)
- [Bourne强化学习笔记2：彻底搞清楚什么是Q-learning与Sarsa](https://blog.csdn.net/linyijiong/article/details/81607691)

## 强化学习中有value-based 和 policy-based，这两种的优缺点分别是什么？应用场景分别是什么？

- [ ] TODO

## value-based方法学习的目标是什么？

- [ ] TODO

## 强化学习 DQN，DDQN，AC，DDPG 的区别

- [ ] TODO

## 参考资料

- [再励学习面试真题](https://zhuanlan.zhihu.com/p/33133828)
- [强化学习面经](https://zhuanlan.zhihu.com/p/44285282)


