[TOC]

# 深度学习

## 神经网络中的Epoch、Iteration、Batchsize

神经网络中epoch与iteration是不相等的

- batch size：中文翻译为批大小（批尺寸）。在深度学习中，一般采用SGD训练，即每次训练在训练集中取batchsize个样本训练；

- iteration：中文翻译为迭代，1个iteration等于使用batchsize个样本训练一次；一个迭代 = 一个正向通过+一个反向通过。iteration = sample size/batch size

- epoch：迭代次数，1个epoch等于使用训练集中的全部样本训练一次；一个epoch = 所有训练样本的一个正向传递和一个反向传递

举个例子，训练集有1000个样本，batchsize=10，那么：训练完整个样本集需要：100次iteration，1次epoch。

![img](https://gss0.baidu.com/-vo3dSag_xI4khGko9WTAnF6hhy/zhidao/wh%3D600%2C800/sign=36204981f1039245a1e0e909b7a488fa/e61190ef76c6a7ef3bf5176af0faaf51f2de66af.jpg)

**参考资料**

- [神经网络中的Epoch、Iteration、Batchsize](https://zhuanlan.zhihu.com/p/67414365)
- [神经网络中epoch与iteration相等吗](https://zhidao.baidu.com/question/716300338908227765.html)

## 反向传播（BP）
![Logistic Regression.png](https://github.com/vivienzou1/Deep-Learning-Interview-Book/blob/master/docs/imgs/00028.png)
![Logistic Regression.png](https://github.com/vivienzou1/Deep-Learning-Interview-Book/blob/master/docs/imgs/00029.png)
![Logistic Regression.png](https://github.com/vivienzou1/Deep-Learning-Interview-Book/blob/master/docs/imgs/00020.png)
![Logistic Regression.png](https://github.com/vivienzou1/Deep-Learning-Interview-Book/blob/master/docs/imgs/00021.png)
![Logistic Regression.png](https://github.com/vivienzou1/Deep-Learning-Interview-Book/blob/master/docs/imgs/00022.png)
![Logistic Regression.png](https://github.com/vivienzou1/Deep-Learning-Interview-Book/blob/master/docs/imgs/00023.png)
![Logistic Regression.png](https://github.com/vivienzou1/Deep-Learning-Interview-Book/blob/master/docs/imgs/00024.png)
![Logistic Regression.png](https://github.com/vivienzou1/Deep-Learning-Interview-Book/blob/master/docs/imgs/00025.png)
![Logistic Regression.png](https://github.com/vivienzou1/Deep-Learning-Interview-Book/blob/master/docs/imgs/00026.png)
![Logistic Regression.png](https://github.com/vivienzou1/Deep-Learning-Interview-Book/blob/master/docs/imgs/00027.png)


**参考资料**

- [一文搞懂反向传播算法](https://www.jianshu.com/p/964345dddb70)

## CNN本质和优势

局部卷积（提取局部特征）:  Local spatial coherence of input & reduece computational complexity: able to reduce dramatically the number of operation needed to process an image by using convolution on patches of adjacent pixels, because adjacent pixels together are meaningful. 

权值共享（降低训练难度）: weight sharing: Sharing weights in this way significantly reduces the number of weights we have to learn, making it easier to learn very deep architectures, and additionally allows us to learn features that are agnostic to what region of the input is being considered.

Pooling（降维，将低层次组合为高层次的特征）

多层次结构

* advantages: compared to ANN
1. automatically detects the important features without any human supervision
2. more accuracy and effective

## saddle point 的定义和特点？
* definition: A point of a function or surface which is a stationary point but not an extremum.
* 1st Derivative = 0, critical point

![Logistic Regression.png](https://github.com/vivienzou1/Deep-Learning-Interview-Book/blob/master/docs/imgs/00030.png)
![Logistic Regression.png](https://github.com/vivienzou1/Deep-Learning-Interview-Book/blob/master/docs/imgs/00031.png)

if H < 0, then (x0,y0) is the saddle point. 
* the Hessian matrix or Hessian: a square matrix of second-order partial derivatives of a scalar-valued function, or scalar field. It describes the local curvature of a function of many variables.

## 神经网络Data Preprocessing有哪些？

1. Mean subtraction: subtracting the mean across every individual feature in the data, and has the geometric interpretation of centering the cloud of data around the origin along every dimension. `X -= np.mean(X)`
2. Normalization
* normalizing the data dimensions so that they are of approximately the same scale. `X /= np.std(X, axis = 0)`
3. PCA and Whitening
- compute the covariance matrix that tells us about the correlation structure
- the covariance matrix is symmetric and positive semi-definite. We can compute the SVD factorization of the data covariance matrix
- reduce dimension
```
# Assume input data matrix X of size [N x D]
X -= np.mean(X, axis = 0) # zero-center the data (important)
cov = np.dot(X.T, X) / X.shape[0] # get the data covariance matrix
U,S,V = np.linalg.svd(cov)
Xrot = np.dot(X, U) # decorrelate the data
Xrot_reduced = np.dot(X, U[:,:100]) # Xrot_reduced becomes [N x 100]
# whiten the data:
# divide by the eigenvalues (which are square roots of the singular values)
Xwhite = Xrot / np.sqrt(S + 1e-5)
```

## 神经网络怎样进行参数初始化？
1. pre-train: fine tuning
2. 对w随机初始化: 当神经网络的层数增多时，会发现越往后面的层的激活函数（使用tanH）的输出值几乎都接近于0. 会导致梯度非常接近于0，gradient exploding and vanishing. 
3. Xavier initialization: linear assumption, 尽可能的让输入和输出服从相同的分布，这样就能够避免后面层的激活函数的输出值趋向于0。var=1/n, 服从[−r,r] 间的均匀分布, var=1/n ，可得，r= (3/n) ^0.5
4. He initialization: ReLu assumption. var=2/n, 服从[−r,r] 间的均匀分布, var=2/n ，可得，r= (6/n)^0.5
5. random initialization with batch normalization: BN reduces the influence of size of initialization, with smaller standard deviation

## 卷积

- see notebook 2 pg 8 - 10

**参考资料**

- [Feature Extraction Using Convolution](http://ufldl.stanford.edu/tutorial/supervised/FeatureExtractionUsingConvolution/)
- [convolution](https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/convolution.html)

- [理解图像卷积操作的意义](https://blog.csdn.net/chaipp0607/article/details/72236892?locationNum=9&fps=1)

- [关于深度学习中卷积核操作](https://www.cnblogs.com/Yu-FeiFei/p/6800519.html)

### 卷积的反向传播过程

![Logistic Regression.png](https://github.com/vivienzou1/Deep-Learning-Interview-Book/blob/master/docs/imgs/00032.png)
![Logistic Regression.png](https://github.com/vivienzou1/Deep-Learning-Interview-Book/blob/master/docs/imgs/00033.png)
![Logistic Regression.png](https://github.com/vivienzou1/Deep-Learning-Interview-Book/blob/master/docs/imgs/00034.png)
![Logistic Regression.png](https://github.com/vivienzou1/Deep-Learning-Interview-Book/blob/master/docs/imgs/00035.png)

- [卷积神经网络(CNN)反向传播算法推导](https://zhuanlan.zhihu.com/p/61898234)

**参考资料**

- [Notes on Convolutional Neural Network](http://cogprints.org/5869/1/cnn_tutorial.pdf)
- [Deep Learning论文笔记之（四）CNN卷积神经网络推导和实现](https://blog.csdn.net/zouxy09/article/details/9993371)

- [反向传导算法](http://deeplearning.stanford.edu/wiki/index.php/%E5%8F%8D%E5%90%91%E4%BC%A0%E5%AF%BC%E7%AE%97%E6%B3%95)

- [Deep learning：五十一(CNN的反向求导及练习)](https://www.cnblogs.com/tornadomeet/p/3468450.html)

- [卷积神经网络(CNN)反向传播算法](https://www.cnblogs.com/pinard/p/6494810.html)

- [卷积神经网络(CNN)反向传播算法公式详细推导](https://blog.csdn.net/walegahaha/article/details/51945421)

- [全连接神经网络中反向传播算法数学推导](https://zhuanlan.zhihu.com/p/61863634)

- [卷积神经网络(CNN)反向传播算法推导](https://zhuanlan.zhihu.com/p/61898234)


## CNN 模型所需的计算力（flops）和参数（parameters）数量是怎么计算的？

对于一个卷积层，假设其大小为  （其中c为#input channel, n为#output channel），输出的feature map尺寸为  ，则该卷积层的

- paras =![](https://www.zhihu.com/equation?tex=n+%5Ctimes+%28h+%5Ctimes+w+%5Ctimes+c+%2B+1%29)

- FLOPs = ![](https://www.zhihu.com/equation?tex=H%27+%5Ctimes+W%27+%5Ctimes+n+%5Ctimes%28h+%5Ctimes+w+%5Ctimes+c+%2B+1%29)


- see notebook 2 Pg. 10

**参考资料**


- [CNN 模型所需的计算力（flops）和参数（parameters）数量是怎么计算的？](https://www.zhihu.com/question/65305385/answer/256845252)
- [CNN中parameters和FLOPs计算](https://blog.csdn.net/sinat_34460960/article/details/84779219)
- [FLOPS理解](https://blog.csdn.net/smallhujiu/article/details/80876875)
- [PyTorch-OpCounter](https://github.com/Lyken17/pytorch-OpCounter)

## 池化（Pooling）
** mean pooling **
![Logistic Regression.png](https://github.com/vivienzou1/Deep-Learning-Interview-Book/blob/master/docs/imgs/00036.png)

** max pooling **
![Logistic Regression.png](https://github.com/vivienzou1/Deep-Learning-Interview-Book/blob/master/docs/imgs/00037.png)

### 池化层怎么接收后面传过来的损失？

- CNN网络中另外一个不可导的环节就是Pooling池化操作，因为Pooling操作使得feature map的尺寸变化，假如做2×2的池化，假设那么第l+1层的feature map有16个梯度，那么第l层就会有64个梯度，这使得梯度无法对位的进行传播下去。其实解决这个问题的思想也很简单，就是把1个像素的梯度传递给4个像素，但是需要保证传递的loss（或者梯度）总和不变。根据这条原则，mean pooling和max pooling的反向传播也是不同的。

**平均池化（Mean Pooling）**

mean pooling的前向传播就是把一个patch中的值求取平均来做pooling，那么反向传播的过程也就是把某个元素的梯度等分为n份分配给前一层，这样就保证池化前后的梯度（残差）之和保持不变，还是比较理解的，图示如下 

![](https://img-blog.csdn.net/20170615205352655?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcXFfMjExOTAwODE=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)


**最大池化（Max Pooling）**

max pooling也要满足梯度之和不变的原则，max pooling的前向传播是把patch中最大的值传递给后一层，而其他像素的值直接被舍弃掉。那么反向传播也就是把梯度直接传给前一层某一个像素，而其他像素不接受梯度，也就是为0。所以max pooling操作和mean pooling操作不同点在于需要记录下池化操作时到底哪个像素的值是最大，也就是max id，这个可以看caffe源码的pooling_layer.cpp，下面是caffe框架max pooling部分的源码

```python

// If max pooling, we will initialize the vector index part.

if (this->layer_param_.pooling_param().pool() == PoolingParameter_PoolMethod_MAX && top.size() == 1)

{

    max_idx_.Reshape(bottom[0]->num(), channels_, pooled_height_,pooled_width_);

  }

```

![](https://img-blog.csdn.net/20170615211413093?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcXFfMjExOTAwODE=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)


**参考资料**

- [如何理解CNN中的池化？](https://zhuanlan.zhihu.com/p/35769417)
- [深度学习笔记（3）——CNN中一些特殊环节的反向传播](https://blog.csdn.net/qq_21190081/article/details/72871704)



## 感受野

### receptive field 计算

The receptive field is defined as the region in the input space that a particular CNN’s feature is looking at.
有一个概念叫做感受野，用来表示网络内部的不同位置的神经元对原图像的感受范围的大小

the closer a pixel to the center of the field, the more it contributes to the calculation of the output feature. a feature does not only look at a particular region (i.e. its receptive field) in the input image, but also focus exponentially more to the middle of that region. 

`(N-1)_RF = f(N_RF, stride, kernel) = (N_RF - 1) * stride + kernel`
RF是感受野。N_RF和RF有点像，N代表 neighbour，指的是第n层的 a feature在n-1层的RF，记住N_RF只是一个中间变量，不要和RF混淆。 stride是步长，ksize是卷积核大小。
```
ksize = 3;
strides = [1 2 2];
N_RF = 1;
for i = 1:3
    
    N_RF = (N_RF -1)*strides(i) + ksize;
    
end
fprintf('The RF is: %d \n',N_RF)
```

**参考资料**

- [卷积神经网络物体检测之感受野大小计算](https://www.cnblogs.com/objectDetect/p/5947169.html)

- [如何计算感受野(Receptive Field)——原理](https://zhuanlan.zhihu.com/p/31004121)


### 卷积神经网络的感受野

- 一般task要求感受野越大越好，如图像分类中最后卷积层的感受野要大于输入图像，网络深度越深感受野越大性能越好
- 密集预测task要求输出像素的感受野足够的大，确保做出决策时没有忽略重要信息，一般也是越深越好
- 目标检测task中设置anchor要严格对应感受野，anchor太大或偏离感受野都会严重影响检测性能

![Logistic Regression.png](https://github.com/vivienzou1/Deep-Learning-Interview-Book/blob/master/docs/imgs/00038.png)

- 公式一是通用计算卷积层输入输出特征图大小的标准公式
- 公式二计算输出特征图的jump，等于输入特征图的jump乘当前卷积层的步进s
- 公式三计算感受野大小，等于输入感受野加当前层的卷积影响因子(k - 1) * jin，注意这里与当前层的步进s没有关系
- 公式四计算输出特征图左上角位置第一个特征向量，对应输入图像感受野的中心位置，注意这里与padding有关系

**参考资料**

- [卷积神经网络的感受野](https://zhuanlan.zhihu.com/p/44106492)

## 权重初始化方法

| truncated normal initializaer | xavier initializer | variance scaling initializer |
| ------------------------------|--------------------|------------------------------|
| `tf.truncated_normal_initializer(stddev=0.01)`| `tf.contrib.layers.xavier_initializer`|`tf.contrib.layers.variance_scaling_initializer`|

### Xavier

| uniform形式| normal形式| 
|-----------|-----------|
| `nn.init.xavier_uniform(tensor, gain = 1)`|`nn.init.xavier_normal(tensor, gain = 1)`|

## 正则化方法

1. [参数范数惩罚](https://baike.baidu.com/item/%E6%AD%A3%E5%88%99%E5%8C%96%E6%96%B9%E6%B3%95/19145625?fr=aladdin#1)
- regularization is the process of adding information in order to solve an ill-posed problem or to prevent overfitting. Regularization applies to objective functions in ill-posed optimization problems.
![Logistic Regression.png](https://github.com/vivienzou1/Deep-Learning-Interview-Book/blob/master/docs/imgs/00039.png)

2. [L2参数正则化](https://baike.baidu.com/item/%E6%AD%A3%E5%88%99%E5%8C%96%E6%96%B9%E6%B3%95/19145625?fr=aladdin#2)
3. [L1参数正则化](https://baike.baidu.com/item/%E6%AD%A3%E5%88%99%E5%8C%96%E6%96%B9%E6%B3%95/19145625?fr=aladdin#3)
4. [L1正则化和L2正则化的区别](https://baike.baidu.com/item/%E6%AD%A3%E5%88%99%E5%8C%96%E6%96%B9%E6%B3%95/19145625?fr=aladdin#4)
5.  [数据集增强](https://baike.baidu.com/item/%E6%AD%A3%E5%88%99%E5%8C%96%E6%96%B9%E6%B3%95/19145625?fr=aladdin#5)
- 另一种数据集增强的方法是向网络的输入层注入噪声。神经网络已被证明对噪声不是非常健壮。简单地将随机噪声施加到输入再进行训练可以改善神经网络的健壮性。对于某些模型，在模型的输入上添加方差极小的噪声等价于对权重施加范数惩罚（Bishop）。
- 研究表明，将噪声施加到网络的隐藏层也是有效的，这可以被看成是在多个抽象层上进行数据集增强。

6.  [噪音的鲁棒性](https://baike.baidu.com/item/%E6%AD%A3%E5%88%99%E5%8C%96%E6%96%B9%E6%B3%95/19145625?fr=aladdin#6)
- 噪声注入远比简单的收缩参数强大，特别是噪声被添加到隐藏单元时。
- 另一种正则化模型的噪声使用方法是将其直接添加到学习到的权重上。权重的贝叶斯推断的一种随机实现。贝叶斯方法认为学习到的模型权重上不确定的，并且这种不确定性可以通过权重的概率分布来表示。添加噪音到学习到的权重上可以看着是反映这种不确定行的一种随机的、实用的方式。

7. [向输出目标注入噪声](https://baike.baidu.com/item/%E6%AD%A3%E5%88%99%E5%8C%96%E6%96%B9%E6%B3%95/19145625?fr=aladdin#7)
8.  [半监督学习](https://baike.baidu.com/item/%E6%AD%A3%E5%88%99%E5%8C%96%E6%96%B9%E6%B3%95/19145625?fr=aladdin#8)
9.  [多任务学习](https://baike.baidu.com/item/%E6%AD%A3%E5%88%99%E5%8C%96%E6%96%B9%E6%B3%95/19145625?fr=aladdin#9)
10. [提前终止](https://baike.baidu.com/item/%E6%AD%A3%E5%88%99%E5%8C%96%E6%96%B9%E6%B3%95/19145625?fr=aladdin#10)
11. [参数绑定和共享](https://baike.baidu.com/item/%E6%AD%A3%E5%88%99%E5%8C%96%E6%96%B9%E6%B3%95/19145625?fr=aladdin#11)
- 参数共享的一个显著优点是，只有参数（独立一个集合）的子集需要被存储在内存中。 对于某些特定模型，如卷积神经网络，这可能可以显著减少模型所占用的内存。
- 参数共享显著降低了CNN模型的参数数量，并显著提高了网络的大小而不需要相应地增加训练数据。
12. [稀疏表示](https://baike.baidu.com/item/%E6%AD%A3%E5%88%99%E5%8C%96%E6%96%B9%E6%B3%95/19145625?fr=aladdin#12)
13. [集成化方法](https://baike.baidu.com/item/%E6%AD%A3%E5%88%99%E5%8C%96%E6%96%B9%E6%B3%95/19145625?fr=aladdin#13)
- 集成化方法是一种通用的降低泛化误差的方法，通过合并多个模型的结果，也叫作模型平均。主要想法是分别训练几个不同的模型，然后让所有模型表决测试样例的输出。
　　经验：原始输入每一个节点选择概率0.8，隐藏层选择概率为0.5。
- Bagging是一种常用的集成学习方法。不同初始化方法、不同mini batch选择方法、不同的超参数选择方法。
- Boosting，通过改变样本权重来训练不同模型。

**参考资料**

- [正则化方法](https://baike.baidu.com/item/%E6%AD%A3%E5%88%99%E5%8C%96%E6%96%B9%E6%B3%95/19145625?fr=aladdin)

## Batch Normalization（BN）

### BN 原理

* Internal Covariate Shift: 
- 在深层网络训练的过程中，由于网络中参数变化而引起内部结点数据分布发生变化. 随着梯度下降的进行，每一层的参数 W_(l) 与 b_(l) 都会被更新，那么 Z_(l) 的分布也就发生了改变，进而 A_(l) 也同样出现分布的改变。而 A_(l) 作为第 l+1 层的输入，意味着 l+1 层就需要去不停适应这种数据分布的变化，这一过程就被叫做Internal Covariate Shift。

* Internal Covariate Shift会带来什么问题？
1. 上层网络需要不停调整来适应输入数据分布的变化，导致网络学习速度的降低
- 我们在上面提到了梯度下降的过程会让每一层的参数 W_(l) 与 b_(l) 发生变化，进而使得每一层的线性与非线性计算结果分布产生变化。后层网络就要不停地去适应这种分布变化，这个时候就会使得整个网络的学习速率过慢。
2. 网络的训练过程容易陷入梯度饱和区，减缓网络收敛速度
- 当我们在神经网络中采用饱和激活函数（saturated activation function）时，例如sigmoid，tanh激活函数，很容易使得模型训练陷入梯度饱和区（saturated regime）。随着模型训练的进行，我们的参数 W_(l) 会逐渐更新并变大，此时 Z_(l) = W_(l)*A_(l-1)+b_(l) 就会随之变大，并且 Z_(l) 还受到更底层网络参数 W_(1), W_(2),...W_(l-1) 的影响，随着网络层数的加深，Z_(l) 很容易陷入梯度饱和区，此时梯度会变得很小甚至接近于0，参数的更新速度就会减慢，进而就会放慢网络的收敛速度。

* Batch Normalization
- BN又引入了两个可学习（learnable）的参数 Beta 与 Alpha 。这两个参数的引入是为了恢复数据本身的表达能力，对规范化后的数据进行线性变换，即 Z_head = Alpha_j*Z_head_j+Beta_j 。特别地，当 Alpha^2 = variacne, Beta = mean 时，可以实现等价变换（identity transform）并且保留了原始输入特征的分布信息。

* Batch Normalization的优势
1. BN使得网络中每层输入数据的分布相对稳定，加速模型学习速度
2. BN使得模型对网络中的参数不那么敏感，简化调参过程，使得网络学习更加稳定
3. BN允许网络使用饱和性激活函数（例如sigmoid，tanh等），缓解梯度消失问题
4. BN具有一定的正则化效果
5. Batch normalization reduces the internal covariate shift (ICS) and accelerates the training of a deep neural network
6. This approach reduces the dependence of gradients on the scale of the parameters or of their initial values which result in higher learning rates without the risk of divergence
7. Batch Normalisation makes it possible to use saturating nonlinearities by preventing the network from getting stuck in the saturated modes

**参考资料**

- [Batch Normalization原理与实战](https://zhuanlan.zhihu.com/p/34879333)

### 手写 BN

```
import numpy as np
x: input list
def batchNormalization(x,gamma, beta, eps = 1e-5):

if len(x.shape) == 2:
    mean = np.mean(x, axis = 0)
    var = np.mean(np.subtract(x,mean)**2, asix = 0)
    x_hat = np.subtract(x,mean)*1./(np.sqrt(var + eps)
    y = gamma * x_hat + beta
    
elif len(x.shape) == 4:
    # the extract the dimensions
    N,C,H,W = x.shape
    # mini-batch mean
    mean = np.mean(x, axis = (0,2,3)
    # mini-batch variance
    var = np.mean((x - mean.reshape((1,C,1,1)))**2, axis = (0,2,3)
    # normalization
    x_hat = np.subtract(x,mean.reshape((1,C,1,1)))*1.0/np.sqrt(var.reshape(1,C,1,1)+eps)
    # result
    y = gamma.reshape(1,C,1,1) * x_hav + beta.reshape(1,C,1,1)
```

### BN 可以防止过拟合么？为什么

- one of BN's possible influence, but not the straight function of BN
- Batch Normalization（以下称BN）的主要作用是加快网络的训练速度。其核心是通过对系统参数搜索空间进行约束来增加系统鲁棒性，这种约束压缩了搜索空间，约束也改善了系统的结构合理性，这会带来一系列的性能改善，比如加速收敛，保证梯度，缓解过拟合等。

### BN 有哪些参数？

gamma, beta, eps(hyperparameter)
![Logistic Regression.png](https://github.com/vivienzou1/Deep-Learning-Interview-Book/blob/master/docs/imgs/00040.png)

### BN 的反向传播推导

![Logistic Regression.png](https://github.com/vivienzou1/Deep-Learning-Interview-Book/blob/master/docs/imgs/00041.png)
![Logistic Regression.png](https://github.com/vivienzou1/Deep-Learning-Interview-Book/blob/master/docs/imgs/00042.png)
![Logistic Regression.png](https://github.com/vivienzou1/Deep-Learning-Interview-Book/blob/master/docs/imgs/00043.png)
![Logistic Regression.png](https://github.com/vivienzou1/Deep-Learning-Interview-Book/blob/master/docs/imgs/00044.png)

### BN 在训练和测试的区别？

- 对于BN，在训练时，是对每一批的训练数据进行归一化，也即用每一批数据的均值和方差。
- 而在测试时，比如进行一个样本的预测，就并没有batch的概念，因此，这个时候用的均值和方差是全量训练数据的均值和方差，这个可以通过移动平均法求得。
- 对于BN，当一个模型训练完成之后，它的所有参数都确定了，包括均值和方差，gamma和bata。

## Weight Normalization（WN）
- 是在权值的维度上做的归一化。WN的做法是将权值向量 w 在其欧氏范数和其方向上解耦成了参数向量 v 和参数标量 g 后使用SGD分别优化这两个参数。
![Logistic Regression.png](https://github.com/vivienzou1/Deep-Learning-Interview-Book/blob/master/docs/imgs/00046.png)
![Logistic Regression.png](https://github.com/vivienzou1/Deep-Learning-Interview-Book/blob/master/docs/imgs/00045.png)
- much faster than batch normalization
- weight normalization is computationally cheaper compared to batch normalization
* advantages
* Weight normalization improves the conditioning of the optimisation problem as well as speed up the convergence of stochastic gradient descent.
* It can be applied successfully to recurrent models such as LSTMs as well as in deep reinforcement learning or generative models

## Layer Normalization（LN）

The key feature of layer normalization is that it normalizes the inputs across the features.
![Logistic Regression.png](https://github.com/vivienzou1/Deep-Learning-Interview-Book/blob/master/docs/imgs/00047.png)
![Logistic Regression.png](https://github.com/vivienzou1/Deep-Learning-Interview-Book/blob/master/docs/imgs/00048.png)
[layer normalization vs batch normalization](https://mlexplained.com/2018/01/13/weight-normalization-and-layer-normalization-explained-normalization-in-deep-learning-part-2/)
* advantages
* Layer normalization can be easily applied to recurrent neural networks by computing the normalization statistics separately at each time step
* This approach is effective at stabilising the hidden state dynamics in recurrent networks 
 
## Instance Normalization（IN）

normalizes across each channel in each training example instead of normalizing across input features in a training example. Unlike batch normalization, the instance normalization layer is applied at test time as well(due to the non-dependency of mini-batch).
![Logistic Regression.png](https://github.com/vivienzou1/Deep-Learning-Interview-Book/blob/master/docs/imgs/00049.png)
* advantages
* This normalization simplifies the learning process of a model.
* The instance normalization can be applied at test time. 

## Group Normalization（GN）

This approach works by dividing the channels into groups and computes within each group the mean and variance for normalization i.e. normalising the features within each group. Unlike batch normalization, group normalization is independent of batch sizes, and also its accuracy is stable in a wide range of batch sizes. 
* advantages
* It has the ability to replace batch normalization in a number of deep learning tasks
* It can be easily implemented in modern libraries with just a few lines of codes

### BN、LN、WN、IN和GN的区别

| Batch Normalization | Layer Normalization | Wegith Normalization | Instance Normalization | Group Normalization |
|---------------------|---------------------|----------------------|------------------------|---------------------|
|对batch size有依赖|避开了batch维度||归一化的维度为[H，W]|GN介于LN和IN之间,其首先将channel分为许多组（group)|
|当batch size较大时，有不错的效果| 归一化的维度为[C，H，W]|||对每一组做归一化，及先将feature的维度由[N, C, H, W]|
|batch的维度上norm||||reshape为[N*G，C//G , H, W]，归一化的维度为[C//G , H, W]|
|归一化维度为[N，H，W]|||||
|对batch中对应的channel归一化|||||



**参考资料**

- [BatchNormalization、LayerNormalization、InstanceNorm、GroupNorm、SwitchableNorm总结](https://blog.csdn.net/liuxiao214/article/details/81037416)
- [UNDERSTANDING NORMALISATION METHODS IN DEEP LEARNING](https://analyticsindiamag.com/understanding-normalization-methods-in-deep-learning/)

## 优化算法

1.  随机梯度下降（SGD）
parameter update for each training example x(i) and label y(i): n 是学习率，J是梯度 SGD完全依赖于当前batch的梯度，所以 n 可理解为允许当前batch的梯度多大程度影响参数更新
![Logistic Regression.png](https://github.com/vivienzou1/Deep-Learning-Interview-Book/blob/master/docs/imgs/00050.png)
* disadvantages
- SGD performs frequent updates with a high variance that cause the objective function to fluctuate heavily
- 选择合适的learning rate比较困难
- SGD容易收敛到局部最优，并且在某些情况下可能被困在saddle point

* advantages:
- SGD 能更有效的利用信息，特别是信息比较冗余的时候
- SGD 在前期迭代效果卓越
- 如果样本数量大，那么 SGD的Computational Complexity 依然有优势
```
for i in range(epoch):
    np.ramdon.shuffle(data)
    for e in data:
        J = evaluate_gradient(loss_function, example, para)
        para = para - J * lr
```
2. Mini-Batch
* advantages:
- reduces the variance of the parameter updates, which can lead to more stable convergence;
- can make use of highly optimized matrix optimizations common to state-of-the-art deep learning libraries that make computing the gradient w.r.t. a mini-batch very efficient. 

* disadvantages:
- Choosing a proper learning rate can be difficult. 
- Learning rate schedules try to adjust the learning rate during training, reducing the learning rate according to a pre-defined schedule or when the change in objective between epochs falls below a threshold. These schedules and thresholds, however, have to be defined in advance and are thus unable to adapt to a dataset's characteristics.
- minimizing highly non-convex error functions common for neural networks is avoiding getting trapped in their numerous suboptimal local minima. 
```
for i in range(epoch):
    np.random.shuffle(data)
    for e in range(data, batch_size=50):
        J = evaluate_gradient(loss_function, batch, para)
        para = para - J * lr
```

3. 动量（Momentum）
- see notebook 2 Pg. 16

4. AdaGrad
* concept:
- adapts the learning rate to the parameters, performing smaller updates (i.e. low learning rates) for parameters associated with frequently occurring features, and larger updates (i.e. high learning rates) for parameters associated with infrequent features. 
- well-suited for dealing with sparse data
- Adagrad uses a different learning rate for every parameter θ_i at every time step t, we first show Adagrad's per-parameter update, which we then vectorize.
![Logistic Regression.png](https://github.com/vivienzou1/Deep-Learning-Interview-Book/blob/master/docs/imgs/00051.png)
- G_t ∈ R d×d  here is a diagonal matrix where each diagonal element i,i is the sum of the squares of the gradients w.r.t. θ_i up to time step t, while ϵ is a smoothing term that avoids division by zero (usually on the order of 1e−8). g_t,i is then the partial derivative of the objective function w.r.t. to the parameter θ_i at time step t.

* 特点：
- 前期g_t较小的时候， regularizer较大，能够放大梯度
- 后期g_t较大的时候，regularizer较小，能够约束梯度
- 适合处理稀疏梯度
- it eliminates the need to manually tune the learning rate
- default value of 0.01 

* 缺点：
- 由公式可以看出，仍依赖于人工设置一个全局学习率
- n设置过大的话，会使regularizer过于敏感，对梯度的调节太大
- 中后期，分母上梯度平方的累加将会越来越大，使gradient ->0，使得训练提前结束

[深度学习最全优化方法总结比较（SGD，Adagrad，Adadelta，Adam，Adamax，Nadam）](https://zhuanlan.zhihu.com/p/22252270)

5. AdaDelta
an extension of Adagrad that seeks to reduce its aggressive, monotonically decreasing learning rate. Adadelta restricts the window of accumulated past gradients to some fixed size w.The running average E[g]^2t at time step t then depends (as a fraction γ similarly to the Momentum term) only on the previous average and the current gradient:
而Adadelta只累加固定大小的项，并且也不直接存储这些项，仅仅是近似计算对应的平均值
![Logistic Regression.png](https://github.com/vivienzou1/Deep-Learning-Interview-Book/blob/master/docs/imgs/00052.png)

* 特点：
- 训练初中期，加速效果不错，很快
- 训练后期，反复在局部最小值附近抖动

6. RMSProp
- Hinton suggests γ to be set to 0.9, while a good default value for the learning rate η is 0.001.
- RMSprop as well divides the learning rate by an exponentially decaying average of squared gradients.
![Logistic Regression.png](https://github.com/vivienzou1/Deep-Learning-Interview-Book/blob/master/docs/imgs/00053.png)

* 特点：
- 其实RMSprop依然依赖于全局学习率
- RMSprop算是Adagrad的一种发展，和Adadelta的变体，效果趋于二者之间
- 适合处理非平稳目标 - 对于RNN效果很好

7. Adam (mini-batch)
- Adam算法结合了Momentum和RMSprop梯度下降法，是一种极其常见的学习算法，被证明能有效适用于不同神经网络，适用于广泛的结构。
- In addition to storing an exponentially decaying average of past squared gradients v_t like Adadelta and RMSprop, Adam also keeps an exponentially decaying average of past gradients m_t, similar to momentum. 
- 它利用梯度的一阶矩估计和二阶矩估计动态调整每个参数的学习率。Adam的优点主要在于经过偏置校正后，每一次迭代学习率都有个确定范围，使得参数比较平稳。
![Logistic Regression.png](https://github.com/vivienzou1/Deep-Learning-Interview-Book/blob/master/docs/imgs/00054.png)

* 特点：
- 结合了Adagrad善于处理稀疏梯度和RMSprop善于处理非平稳目标的优点
- 对内存需求较小
- 为不同的参数计算不同的自适应学习率
- 也适用于大多非凸优化 - 适用于大数据集和高维空间

8. Adamax
![Logistic Regression.png](https://github.com/vivienzou1/Deep-Learning-Interview-Book/blob/master/docs/imgs/00055.png)

9. Nadam
![Logistic Regression.png](https://github.com/vivienzou1/Deep-Learning-Interview-Book/blob/master/docs/imgs/00056.png)
- [AMSGrad](http://ruder.io/optimizing-gradient-descent/index.html#amsgrad)

* 经验之谈
1. 对于稀疏数据，尽量使用学习率可自适应的优化方法，不用手动调节，而且最好采用默认值
2. SGD通常训练时间更长，但是在好的初始化和学习率调度方案的情况下，结果更可靠
3. 如果在意更快的收敛，并且需要训练较深较复杂的网络时，推荐使用学习率自适应的优化方法。
4. Adadelta，RMSprop，Adam是比较相近的算法，在相似的情况下表现差不多。
5. 在想使用带动量的RMSprop，或者Adam的地方，大多可以使用Nadam取得更好的效果
![Logistic Regression.png](https://github.com/vivienzou1/Deep-Learning-Interview-Book/blob/master/docs/imgs/00057.png)
![Logistic Regression.png](https://github.com/vivienzou1/Deep-Learning-Interview-Book/blob/master/docs/imgs/00058.png)


**参考资料**

- [《Deep Learning》第八章：深度模型中的优化](https://exacity.github.io/deeplearningbook-chinese/Chapter8_optimization_for_training_deep_models/)

- [从 SGD 到 Adam —— 深度学习优化算法概览(一)](https://zhuanlan.zhihu.com/p/32626442)
- [Adam 究竟还有什么问题 —— 深度学习优化算法概览(二)](https://zhuanlan.zhihu.com/p/37269222)
- [An overview of gradient descent optimization algorithms](http://ruder.io/optimizing-gradient-descent/)

### 梯度下降法
- see notebook 2 Pg. 15-16

* 缺點
- 靠近局部極小值時速度減慢。
- 直線搜索可能會產生一些問題。
- 可能會「之字型」地下降。

[深入浅出--梯度下降法及其实现](https://www.jianshu.com/p/c7e642877b0e)

### 随机梯度下降法（SGD）

#### SGD每步做什么，为什么能online learning？

- 每次估计梯度的时候， 只选用一个训练样本。
- 之所以称为stochastic， 是因为我们训练之前Randomly shuffle examples in the training set. 这相当于我们随机的选取一个exanple 进行online 的trainging。 但是这也是有区别的， 因为每次训练的时候都会逐次的选取一个样本，注意随机shuffle 之后， 我们取样本的次序就固定了。其中B = 1 的时候， 就是一般的online gradient descent.
- B = the training set size , 这称为standard(also called batch) gradient descent。 注意此时就不能称为stochastic gradient descent 了， 因为毫无随机可言， 这是确定的梯度下降

- [简述动量Momentum梯度下降](https://blog.csdn.net/yinruiyang94/article/details/77944338)

## 激活函数

- convert a input signal of a node in a A-NN to an output signal. 
- They introduce non-linear properties to our Network
- differentiable

**参考资料**

- [What is activate function?](https://yogayu.github.io/DeepLearningCourse/03/ActivateFunction.html)
- [资源 | 从ReLU到Sinc，26种神经网络激活函数可视化](https://mp.weixin.qq.com/s/7DgiXCNBS5vb07WIKTFYRQ)

### Sigmoid

* con: 
- Vanishing gradient problem
- Secondly , its output isn’t zero centered. It makes the gradient updates go too far in different directions. 0 < output < 1, and it makes optimization harder.
- Sigmoids saturate and kill gradients.
- Sigmoids have slow convergence.

#### Sigmoid用作激活函数时，分类为什么要用交叉熵损失，而不用均方损失？

- the two end of sigmoid is very flat, loss function update very slow.

### tanh

- -1 < output < 1
- optimization is easier in this method hence in practice it is always preferred over Sigmoid function . 
- But still it suffers from Vanishing gradient problem.

### ReLU

- avoids and rectifies vanishing gradient problem
- only be used within Hidden layers of a Neural Network Model.
- some gradients can be fragile during training and can die. It can cause a weight update which will makes it never activate on - any data point again. Simply saying that ReLu could result in Dead Neurons.

ReLU 相关变体

#### ReLU 激活函数为什么比sigmoid和tanh好？ 

- reduce or avoid gradient vanishing 

#### ReLU 激活函数为什么能解决梯度消失问题？
1. sigmoid函数的gradient会随着x增大或减小和消失;
2. ReLU函数不会， ReLU(x)′={0,ifx<0; 1,ifx>0} ReLU(x)′={0,ifx<0; 1,ifx>0}
3. 故 Rectified Linear Unit在神经网络中不会产生vanishing gradient 问题

#### ReLU 有哪些变体？

Leaky Rule 

## Dropout

* 防止过拟合的方法：

1. 提前终止（当验证集上的效果变差的时候）
2. L1和L2正则化加权
3. soft weight sharing
4. dropout

### Dropout concept
* dropout率的选择
1. 经过交叉验证，隐含节点dropout率等于0.5的时候效果最好，原因是0.5的时候dropout随机生成的网络结构最多。
2. dropout也可以被用作一种添加噪声的方法，直接对input进行操作。输入层设为更接近1的数。使得输入变化不会太大（0.8）

* pro:
1. save training time
2. reduce overfitting
 - 取平均的作用
  - 减少神经元之间复杂的共适应关系

* concept: 
- dropout也能达到同样的效果，它强迫一个神经单元，和随机挑选出来的其他神经单元共同工作，达到好的效果。消除减弱了神经元节点间的联合适应性，增强了泛化能力。
- 文本分类上，dropout效果提升有限，分析原因可能是Reuters-RCV1数据量足够大，过拟合并不是模型的主要问题.(- maxout 神经网络中得另一种方法，Cifar-10上超越dropout)

* how to use:
- dropout、max-normalization、large decaying learning rates and high momentum组合起来效果更好，比如max-norm regularization就可以防止大的learning rate导致的参数blow up
- 数据量小的时候，dropout效果不好，数据量大了，dropout效果好

**参考资料**

- [理解dropout](https://blog.csdn.net/stdcoutzyx/article/details/49022443)

### Dropout 如何实现？
![Logistic Regression.png](https://github.com/vivienzou1/Deep-Learning-Interview-Book/blob/master/docs/imgs/00059.png)
1. 首先随机（临时）删掉网络中一半的隐藏神经元，输入输出神经元保持不变
2. 然后把输入x通过修改后的网络前向传播，然后把得到的损失结果通过修改的网络反向传播。一小批训练样本执行完这个过程后，在没有被删除的神经元上按照随机梯度下降法更新对应的参数（w，b）。
3. 然后继续重复这一过程：
恢复被删掉的神经元（此时被删除的神经元保持原样，而没有被删除的神经元已经有所更新）
从隐藏层神经元中随机选择一个一半大小的子集临时删除掉（备份被删除神经元的参数）。
对一小批训练样本，先前向传播然后反向传播损失并根据随机梯度下降法更新参数（w，b） （没有被删除的那一部分参数得到更新，删除的神经元参数保持被删除前的结果）。

[深度学习中Dropout原理解析](https://zhuanlan.zhihu.com/p/38200980)

### Drop 在训练和测试的区别

* 训练过程:
1. 对参数w的训练进行球形限制(max-normalization)，对dropout的训练非常有用。
2. 球形半径c是一个需要调整的参数。可以使用验证集进行参数调优
3. dropout自己虽然也很牛，但是dropout、max-normalization、large decaying learning rates and high momentum组合起来效果更好，比如max-norm regularization就可以防止大的learning rate导致的参数blow up。
4. 使用pretraining方法也可以帮助dropout训练参数，在使用dropout时，要将所有参数都乘以1/p。

## 损失函数（Loss）

- difference between pridicted value and real value  
- An optimization problem seeks to minimize a loss function.

### Cross Entropy Loss

- see notebook 1 pg 44

- Cross-entropy is a measure from the field of information theory, building upon entropy and generally calculating the difference between two probability distributions
- KL divergence: **relative** entropy between two probability distributions
- cross-entropy: **total** entropy between the distributions.
- log loss: two measures are derived from a **different source**.

- Skewed Probability Distribution (unsurprising): Low entropy.
- Balanced Probability Distribution (surprising): High entropy.

* application:
1. convolution layer
2. log regression
3. binary

### Hinge Loss

`L(y) = max(0 , 1 – t⋅y)`
y是预测值(-1到1之间)，t为目标值(1或 -1)

* application:
1. SVM
2. 可用于“最大间隔(max-margin)”分类 

### Focal Loss

Focal loss主要是为了解决one-stage目标检测中正负样本比例严重失衡的问题。该损失函数降低了大量简单负样本在训练中所占的权重，也可理解为一种困难样本挖掘。
![Logistic Regression.png](https://github.com/vivienzou1/Deep-Learning-Interview-Book/blob/master/docs/imgs/00060.png)
![Logistic Regression.png](https://github.com/vivienzou1/Deep-Learning-Interview-Book/blob/master/docs/imgs/00061.png)

## 1*1 卷积有什么作用？
1*1卷积过滤器 和正常的过滤器一样，唯一不同的是它的大小是1*1，没有考虑在前一层局部信息之间的关系。最早出现在 Network In Network的论文中 ，使用1*1卷积是想加深加宽网络结构 ，在Inception网络（ Going Deeper with Convolutions ）中用来降维，如下图：

* function:
1. 降维/升维（ channels reductionality/increase ）。比如，一张500 * 500且厚度depth为100 的图片在20个filter上做1*1的卷积，那么结果的大小为500*500*20。
2. 加入非线性。卷积层之后经过激励层，1*1的卷积在前一层的学习表示上添加了非线性激励（ non-linear activation ），提升网络的表达能力；


## AlexNet
- **Dropout**
- 使用ReLU激活函数
- image agumentation
- 多GPU并行训练
- 局部响应标准化: ReLU有个好的特点，就是它不要求输入标准化以防止饱和。因为只要神经元产生正输入，这个神经元就能起到学习的作用。但是有一点，局部标准化是有助于泛化的。具体的标准化公式在论文中给出了，此处不作重点阐述。作者在论文中提到，使用这种标准化，top-1和top-5的错误率都有所降低。
- 重叠池化: 一般的Pooling是不重叠的，而AlexNet使用的Pooling是可重叠的，也就是说，在池化的时候，每次移动的步长小于池化的边长。AlexNet池化的大小为3*3的正方形，每次池化移动步长为2，这样就会出现重叠。这个方案同样降低了top-1和top-5的错误率，同时有效缓解了过拟合。



先给出AlexNet的一些参数和结构图： 

卷积层：5层 

全连接层：3层 

深度：8层 

参数个数：60M 

神经元个数：650k 

分类数目：1000类

**参考资料**

[AlexNet](https://dgschwend.github.io/netscope/#/preset/alexnet)

## VGG

**《Very Deep Convolutional Networks for Large-Scale Image Recognition》**

- arXiv：https://arxiv.org/abs/1409.1556
- intro：ICLR 2015
- homepage：http://www.robots.ox.ac.uk/~vgg/research/very_deep/

[VGG](https://arxiv.org/abs/1409.1556) VGG有两种结构，分别是VGG16和VGG19，两者并没有本质上的区别，只是网络深度不一样。

VGG16相比AlexNet的一个改进是采用**连续的几个3x3的卷积核代替AlexNet中的较大卷积核（11x11，7x7，5x5）**。对于给定的感受野（与输出有关的输入图片的局部大小），**采用堆积的小卷积核是优于采用大的卷积核，因为多层非线性层可以增加网络深度来保证学习更复杂的模式，而且代价还比较小（参数更少）**。

简单来说，在VGG中，使用了3个3x3卷积核来代替7x7卷积核，使用了2个3x3卷积核来代替5*5卷积核，这样做的主要目的是在保证具有相同感知野的条件下，提升了网络的深度，在一定程度上提升了神经网络的效果。

比如，3个步长为1的3x3卷积核的一层层叠加作用可看成一个大小为7的感受野（其实就表示3个3x3连续卷积相当于一个7x7卷积），其参数总量为 3x(9xC^2) ，如果直接使用7x7卷积核，其参数总量为 49xC^2 ，这里 C 指的是输入和输出的通道数。很明显，27xC^2小于49xC^2，即减少了参数；而且3x3卷积核有利于更好地保持图像性质。

这里解释一下为什么使用2个3x3卷积核可以来代替5*5卷积核：

5x5卷积看做一个小的全连接网络在5x5区域滑动，我们可以先用一个3x3的卷积滤波器卷积，然后再用一个全连接层连接这个3x3卷积输出，这个全连接层我们也可以看做一个3x3卷积层。这样我们就可以用两个3x3卷积级联（叠加）起来代替一个 5x5卷积。

具体如下图所示：

![](imgs/DLIB-0012.png)

至于为什么使用3个3x3卷积核可以来代替7*7卷积核，推导过程与上述类似，大家可以自行绘图理解。

下面是VGG网络的结构（VGG16和VGG19都在）：

![VGG](https://d2mxuefqeaa7sj.cloudfront.net/s_8C760A111A4204FB24FFC30E04E069BD755C4EEFD62ACBA4B54BBA2A78E13E8C_1491022251600_VGGNet.png)

- VGG16包含了16个隐藏层（13个卷积层和3个全连接层），如上图中的D列所示
- VGG19包含了19个隐藏层（16个卷积层和3个全连接层），如上图中的E列所示

VGG网络的结构非常一致，从头到尾全部使用的是3x3的卷积和2x2的max pooling。

如果你想看到更加形象化的VGG网络，可以使用[经典卷积神经网络（CNN）结构可视化工具](https://mp.weixin.qq.com/s/gktWxh1p2rR2Jz-A7rs_UQ)来查看高清无码的[VGG网络](https://dgschwend.github.io/netscope/#/preset/vgg-16)。

**VGG优点：**

- VGGNet的结构非常简洁，整个网络都使用了同样大小的卷积核尺寸（3x3）和最大池化尺寸（2x2）。
- 几个小滤波器（3x3）卷积层的组合比一个大滤波器（5x5或7x7）卷积层好：
- 验证了通过不断加深网络结构可以提升性能。

**VGG缺点**：

VGG耗费更多计算资源，并且使用了更多的参数（这里不是3x3卷积的锅），导致更多的内存占用（140M）。其中绝大多数的参数都是来自于第一个全连接层。VGG可是有3个全连接层啊！

PS：有的文章称：发现这些全连接层即使被去除，对于性能也没有什么影响，这样就显著降低了参数数量。

注：很多pretrained的方法就是使用VGG的model（主要是16和19），VGG相对其他的方法，参数空间很大，最终的model有500多m，AlexNet只有200m，GoogLeNet更少，所以train一个vgg模型通常要花费更长的时间，所幸有公开的pretrained model让我们很方便的使用。

关于感受野：

假设你一层一层地重叠了3个3x3的卷积层（层与层之间有非线性激活函数）。在这个排列下，第一个卷积层中的每个神经元都对输入数据体有一个3x3的视野。

**代码篇：VGG训练与测试**

这里推荐两个开源库，训练请参考[tensorflow-vgg](https://github.com/machrisaa/tensorflow-vgg)，快速测试请参考[VGG-in TensorFlow](https://www.cs.toronto.edu/~frossard/post/vgg16/)。

代码我就不介绍了，其实跟上述内容一致，跟着原理看code应该会很快。我快速跑了一下[VGG-in TensorFlow](https://www.cs.toronto.edu/~frossard/post/vgg16/)，代码亲测可用，效果很nice，就是model下载比较烦。

贴心的Amusi已经为你准备好了[VGG-in TensorFlow](https://www.cs.toronto.edu/~frossard/post/vgg16/)的测试代码、model和图像。需要的同学可以关注CVer微信公众号，后台回复：VGG。

天道酬勤，还有很多知识要学，想想都刺激~Fighting！

**参考资料**

- [《Very Deep Convolutional Networks for Large-Scale Image Recognition》](https://arxiv.org/abs/1409.1556)
- [深度网络VGG理解](https://blog.csdn.net/wcy12341189/article/details/56281618)

- [深度学习经典卷积神经网络之VGGNet](https://blog.csdn.net/marsjhao/article/details/72955935)
- [VGG16 结构可视化](https://dgschwend.github.io/netscope/#/preset/vgg-16)

- [tensorflow-vgg](https://github.com/machrisaa/tensorflow-vgg)

- [VGG-in TensorFlow](https://www.cs.toronto.edu/~frossard/post/vgg16/)

- [机器学习进阶笔记之五 | 深入理解VGG\Residual Network](https://zhuanlan.zhihu.com/p/23518167)


## ResNet

**1.ResNet意义**

随着网络的加深，出现了训练集准确率下降的现象，我们可以确定这不是由于Overfit过拟合造成的(过拟合的情况训练集应该准确率很高)；所以作者针对这个问题提出了一种全新的网络，叫深度残差网络，它允许网络尽可能的加深，其中引入了全新的结构如图1； 
这里问大家一个问题 

残差指的是什么？ 

其中ResNet提出了两种mapping：一种是identity mapping，指的就是图1中”弯弯的曲线”，另一种residual mapping，指的就是除了”弯弯的曲线“那部分，所以最后的输出是 y=F(x)+x 
identity mapping顾名思义，就是指本身，也就是公式中的x，而residual mapping指的是“差”，也就是y−x，所以残差指的就是F(x)部分。 

为什么ResNet可以解决“随着网络加深，准确率不下降”的问题？ 

理论上，对于“随着网络加深，准确率下降”的问题，Resnet提供了两种选择方式，也就是identity mapping和residual mapping，如果网络已经到达最优，继续加深网络，residual mapping将被push为0，只剩下identity mapping，这样理论上网络一直处于最优状态了，网络的性能也就不会随着深度增加而降低了。

**2.ResNet结构**

它使用了一种连接方式叫做“shortcut connection”，顾名思义，shortcut就是“抄近道”的意思，看下图我们就能大致理解： 

**ResNet的F(x)究竟长什么样子？**

**参考资料**

- [ResNet解析](https://blog.csdn.net/lanran2/article/details/79057994)
- [ResNet论文笔记](https://blog.csdn.net/wspba/article/details/56019373)

- [残差网络ResNet笔记](https://www.jianshu.com/p/e58437f39f65)
- [Understand Deep Residual Networks — a simple, modular learning framework that has redefined state-of-the-art](https://blog.waya.ai/deep-residual-learning-9610bb62c355)

- [An Overview of ResNet and its Variants](https://towardsdatascience.com/an-overview-of-resnet-and-its-variants-5281e2f56035)     


- [译文](https://www.jianshu.com/p/46d76bd56766)
- [Understanding and Implementing Architectures of ResNet and ResNeXt for state-of-the-art Image Classification: From Microsoft to Facebook [Part 1]](https://medium.com/@14prakash/understanding-and-implementing-architectures-of-resnet-and-resnext-for-state-of-the-art-image-cf51669e1624)
- [给妹纸的深度学习教学(4)——同Residual玩耍](https://zhuanlan.zhihu.com/p/28413039)
- [Residual Networks 理解](https://zhuanlan.zhihu.com/p/32173684)
- [Identity Mapping in ResNet](https://zhuanlan.zhihu.com/p/32206896)
- [resnet（残差网络）的F（x）究竟长什么样子？](https://www.zhihu.com/question/53224378)

### ResNet为什么不用Dropout?

Dropout与BN不兼容的相关论文及说明；同时，BN在训练过程对每个单个样本的forward均引入多个样本（Batch个）的统计信息，相当于自带一定噪音，起到正则效果，所以也就基本消除了Dropout的必要。BN和Dropout同时使用会使精度下降。

**参考资料**

- <https://www.zhihu.com/question/325139089>
- https://zhuanlan.zhihu.com/p/60923972

### 为什么 ResNet 不在一开始就使用residual block,而是使用一个7×7的卷积？

先上一下paper里的图例：

![](F:/Projects/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E9%9D%A2%E8%AF%95%E5%AE%9D%E5%85%B8%E8%B5%84%E6%96%99%E5%A4%A7%E5%85%A8/GitHub/Deep-Learning-Interview-Book/docs/imgs/DLIB-0023.jpg)

原因: 7x7卷积实际上是用来直接对**输入图片**降采样(early downsampling), 注意像7x7这样的大卷积核一般只出现在**input layer**

**目的是:**  尽可能**保留原始图像的信息,** 而不需要增加channels数.

**本质上是:** 多channels的非线性激活层是非常昂贵的, 在**input laye**r用**big kernel**换多channels是划算的

注意一下, resnet接入residual block前pixel为56x56的layer, channels数才**64**, 但是同样大小的layer, 在vgg-19里已经有**256**个channels了.

这里要强调一下, 只有在input layer层, 也就是**最靠近输入图片**的那层, 才用大卷积, 原因如下:

深度学习领域, 有一种广泛的直觉，即更大的卷积更好，但更昂贵。输入层中的特征数量(224x224)是如此之小（相对于隐藏层），第一卷积可以非常大而不会大幅增加实际的权重数。**如果你想在某个地方进行大卷积，第一层通常是唯一的选择**。

我认为神经网络的第一层是最基本的，因为它基本上只是将数据嵌入到一个新的更大的向量空间中。ResNet在第二层之前没有开始其特征层跳过，所以看起来作者想要在开始整花里胡哨的layers之前尽可能保留图像里更多的primary features.

题外话, 同时期的GoogLeNet也在input layer用到了7x7大卷积, 所以resnet作者的灵感来源于GoogLeNet也说不定, 至于非要追问为啥这么用, 也许最直接的理由就是"深度学习就像炼丹, 因为这样网络工作得更好, 所以作者就这么用了". 

再说个有趣的例子, resnet模型是实验先于理论, 实验证明有效, 后面才陆续有人研究为啥有效, 比如[The Shattered Gradients Problem: If resnets are the answer, then what is the question?](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1702.08591)  可不就是炼丹么?

**参考资料**

- [为什么resnet不在一开始就使用residual block,而是使用一个7×7的卷积？](https://www.zhihu.com/question/330735327/answer/725695411)

### 什么是Bottlenet layer？

- 输入输出维度差距较大，就像一个瓶颈一样，上窄下宽亦或上宽下窄

![Logistic Regression.png](https://github.com/vivienzou1/Deep-Learning-Interview-Book/blob/master/docs/imgs/00062.png)

- The three layers are 1X1, 3X3, and 1X1 convolutions, where the 1X1 layers are responsible for reducing and then increasing(restoring) dimensions, leaving the 3X3 layer a bottleneck with smaller input/output dimensions.
- 我们看到，使用 1X1 的网络结构很方便改变维度。灵活设计网络，并且减小计算量。
- **Bottleneck features** are generated from a multi-layer perceptron in which one of the internal layers has a small number of hidden units, relative to the size of the other layers.

### ResNet如何解决梯度消失？

![Logistic Regression.png](https://github.com/vivienzou1/Deep-Learning-Interview-Book/blob/master/docs/imgs/00063.png)
![Logistic Regression.png](https://github.com/vivienzou1/Deep-Learning-Interview-Book/blob/master/docs/imgs/00064.png)

### ResNet网络越来越深，准确率会不会提升？
- yes, reduce gradient vanish by 1) short cut 2) bottle neck
- 从 50-layer 起，ResNet 采用了一种 bottleneck design 的手段。
- 1x1 的卷积核让整个残差单元变得更加细长，这也是 bottleneck 的含义，更重要的是参数减少了。

## ResNet v2

![resnetv2.png](https://github.com/vivienzou1/Deep-Learning-Interview-Book/blob/master/docs/imgs/00065.png)

**参考资料**

- [《Identity Mappings in Deep Residual Networks》](https://arxiv.org/abs/1603.05027)
- [Feature Extractor[ResNet v2]](https://www.cnblogs.com/shouhuxianjian/p/7770658.html)
- [ResNetV2：ResNet深度解析](https://blog.csdn.net/lanran2/article/details/80247515)
- [ResNet v2论文笔记](https://blog.csdn.net/u014061630/article/details/80558661)
- [[ResNet系] 002 ResNet-v2](https://segmentfault.com/a/1190000011228906)

### ResNet v1 与 ResNet v2的区别

作者通过研究ResNet残差学习单元的传播公式，发现前馈和反馈信号可以直接传输，因此skip connection的非线性激活函数（如ReLU）替换为Identity Mappings（）。同时，ResNet V2在每一层中都使用了Batch Normalization。这样处理之后，新的残差学习单元将比以前更容易训练且泛化性更强。

### ResNet v2 的 ReLU 激活函数有什么不同？

* pre-activation
- 为了构建f(yl)=yl成为恒等映射，我们将激活函数（ReLU和BN）移到权值层之前，形成一种“预激活（pre-activation）”的方式，而不是常规的“后激活（post-activation）”方式
- 更容易训练并且泛化性能更好

[ResNet-v2](https://zhuanlan.zhihu.com/p/29678910)

## ResNeXt

![resnetv2.png](https://github.com/vivienzou1/Deep-Learning-Interview-Book/blob/master/docs/imgs/00066.png)

- ResNext 看起来和 Inception net 中的 Inception 模块非常相似，它们都遵循了「分割-转换-合并」的范式。不过在 ResNext 中，multiple paths and use summation rather than concatenation to merge。另外一个区别是，Inception net 中的每一个路径互不相同（1x1、3x3 和 5x5 卷积），而在 ResNeXt 架构中，所有的路径都遵循相同的拓扑结构。

- 引入了一个叫作 **cardinality** 的超参数，指独立路径的数量，通过扩大基数值（而不是深度或宽度），准确率得到了高效提升。作者表示，与 Inception 相比，这个全新的架构更容易适应新的数据集或任务，因为它只有一个简单的范式和一个需要调整的超参数，而 Inception 需要调整很多超参数（比如每个路径的卷积层内核大小）。

- 在不增加参数复杂度的前提下提高准确率，同时还减少了超参数的数量

* pro:
- 与 ResNet 相比，相同的参数个数，ResNeXt 结果更好；
- 或者说同样的效果，ResNeXt的计算量更少，一个 50 层的 ResNeXt 网络，和 101 层的 ResNet 准确度接近。
- 增大Cardinality比增大模型的width或者depth效果更好；
- ResNeXt 网络结构更加简单，仅需要少量的超参数来描述；

**参考资料**

- [ResNeXt算法详解](https://blog.csdn.net/u014380165/article/details/71667916)
- [一文简述ResNet及其多种变体](https://www.jiqizhixin.com/articles/042201)

## Inception系列（V1-V4）

* why inception:
- VGG has to many parameters too slow

### InceptionV1 (GoogLeNet)

- 参数量也是远小于VGG
- concept: 基本组成结构有四个成分。1*1卷积，3*3卷积，5*5卷积，3*3最大池化。最后对四个成分运算结果进行通道上组合。通过多个卷积核提取图像不同尺度的信息，最后进行融合，可以得到图像更好的表征。信息分布比较全局性的图像采用大卷积核，信息分布比较局部性的图像采用小卷积核。

![resnetv2.png](https://github.com/vivienzou1/Deep-Learning-Interview-Book/blob/master/docs/imgs/00067.png)
![resnetv2.png](https://github.com/vivienzou1/Deep-Learning-Interview-Book/blob/master/docs/imgs/00068.png)

* structure: 
1. 采用模块化结构，方便增添和修改。其实网络结构就是叠加Inception Module
2. Averagepool来代替全连接层的思想。实际在最后一层还是添加了一个全连接层，是为了大家做finetune。
3. Dropout层，防止过拟合。
4. 增加了两个辅助的softmax分支，作用有两点，一是为了避免梯度消失，用于向前传导梯度。二是将中间某一层输出用作分类，起到模型融合作用。最后的loss=loss_2 + 0.3 * loss_1 + 0.3 * loss_0。实际测试时，这两个辅助softmax分支会被去掉。

* pro:
1. 保持相同感受野的同时减少参数
2. 加强非线性的表达能力

* 使用辅助分类器： 其实在第一篇论文中GoogLeNet中就使用了辅助分类器，使用了2个，那么它的优势就是

1. 把梯度有效的传递回去，不会有梯度消失问题，加快了训练
2. 中间层的特征也有意义，空间位置特征比较丰富，有利于提成模型的判别力

### InceptionV2

![resnetv2.png](https://github.com/vivienzou1/Deep-Learning-Interview-Book/blob/master/docs/imgs/00069.png)
![resnetv2.png](https://github.com/vivienzou1/Deep-Learning-Interview-Book/blob/master/docs/imgs/00070.png)
- 用两个3*3卷积代替5*5卷积，可以降低参数量
- 提出BN算法

* pro:
1. 提出神经网络结构的设计和优化思路
2. 改进Inception

### InceptionV3
* 设计准则：
1. 避免网络表达瓶颈，尤其在网络的前端。feature map急剧减小，这样对层的压缩过大，会损失大量信息，模型训练困难
2. 高维特征的局部处理更困难
3. 在较低维度空间聚合，不会损失表达能力。
4. 平衡网络的宽度和深度
5. 学习Factorization into small convolutions的思想，将一个二维卷积拆分成两个较小卷积，例如将7*7卷积拆成1*7卷积和7*1卷积。这样做的好处是降低参数量。paper中指出，通过这种非对称的卷积拆分，比对称的拆分为几个相同的卷积效果更好，可以处理更多，更丰富的空间特征。

* 优点：

1. 节约了大量的参数
2. 增加一层非线性，提高模型的表达能力
3. 可以处理更丰富的空间特征，增加特征的多样性

### InceptionV4
- Resnet + inception 
1. Residual Connection

- 作者认为残差连接并不是深度网络所必须的（PS：ResNet的作者说残差连接时深度网络的标配），没有残差连接的网络训练起来并不困难，因为有好的初始化以及Batch Normalization，但是它确实可以大大的提升网路训练的速度。

![resnetv2.png](https://github.com/vivienzou1/Deep-Learning-Interview-Book/blob/master/docs/imgs/00073.png)
![resnetv2.png](https://github.com/vivienzou1/Deep-Learning-Interview-Book/blob/master/docs/imgs/00074.png)

4. 网络精度提高原因

- 残差连接只能加速网络收敛，真正提高网络精度的还是“更大的网络规模”。

**参考资料**

- [一文概览Inception家族的「奋斗史」](https://baijiahao.baidu.com/s?id=1601882944953788623&wfr=spider&for=pc)
- [inception-v1,v2,v3,v4----论文笔记](https://blog.csdn.net/weixin_39953502/article/details/80966046)


## DenseNet
- DenseNet121/169/201/264

* pro:
1. 在less parameters和computational cost with less error than ResNet
2. 缓解了梯度消失问题 
3. 增强了特征在网络间的传播 (concatenating，即维度上的叠加)
4. feature reuse (concatenating)
5. 有效减少了参数数量(concatenating)

* structure
1. 前面所有层与后面层的密集连接（dense connection）: 即互相连接所有的层，具体来说就是每个层都会接受其前面所有层作为其额外的输入。
2. 通过特征在channel上的连接来实现特征重用（feature reuse)
3. 多个紧密连接的密集块（dense blocks），在同一个block保持feature map大小相同，然后在块之间引入过渡层（transition layers）,这个层的构成很简单：BN层+1x1卷积层+2x2平均池化层，通过过渡层，实现下采样。
4. (密集连接的结构，使得每一层都可以直接从损失函数和原始输入信号获得梯度，对于训练更深的网络十分有利，同时作者还提出，密集连接的网络结构有正则化的效果，能够减少过拟合风险。)

* 优势：
1. 由于密集连接方式，DenseNet提升了梯度的反向传播，使得网络更容易训练。由于每层可以直达最后的误差信号，实现了隐式的“deep supervision”；
2. 参数更小且计算更高效，这有点违反直觉，由于DenseNet是通过concat特征来实现短路连接，实现了特征重用，并且采用较小的growth rate，每个层所独有的特征图是比较小的；
3. 由于特征复用，最后的分类器使用了低级特征。

### 为什么 DenseNet 比 ResNet 好？
![resnetv2.png](https://github.com/vivienzou1/Deep-Learning-Interview-Book/blob/master/docs/imgs/00075.png)
![resnetv2.png](https://github.com/vivienzou1/Deep-Learning-Interview-Book/blob/master/docs/imgs/00076.png)
![resnetv2.png](https://github.com/vivienzou1/Deep-Learning-Interview-Book/blob/master/docs/imgs/00078.png)
* dense connection
1. 如果用公式表示的话，传统的网络在 l 层的输出为：`x_l = H_l(x_l-1)`
2. 在DenseNet中，会连接前面所有层作为输入：`x_l = H_l([x_0, x_1, x_2, ..., x_l-1])`
3. 每个层都会与前面所有层在channel维度上连接（concat）在一起（这里各个层的特征图大小是相同的，后面会有说明），并作为下一层的输入。对于一个 L 层的网络，DenseNet共包含 L(L+1)/2 个连接，相比ResNet，这是一种密集连接。而且DenseNet是直接concat来自不同层的特征图，这可以实现特征重用，提升效率，这一特点是DenseNet与ResNet最主要的区别。

### 为什么 DenseNet 比 ResNet 更耗显存？
- 为了确保网络中各层之间的最大信息流，我们将所有图层（具有相同尺寸的特征图）直接相互连接。在连接时，ResNets是通过求和方式进入下一层网络，而DenseNet通过concatenating方式，这也是为什么DenseNet节省参数但耗内存的原因
- DenseNet共包含 L(L+1)/2 个连接

![resnetv2.png](https://github.com/vivienzou1/Deep-Learning-Interview-Book/blob/master/docs/imgs/00079.png)
- Memory-Efficient Implementation of DenseNets。

## SE-Net
![resnetv2.png](https://github.com/vivienzou1/Deep-Learning-Interview-Book/blob/master/docs/imgs/00080.png)
![resnetv2.png](https://github.com/vivienzou1/Deep-Learning-Interview-Book/blob/master/docs/imgs/00083.png)
- feature recalibration: 于通过网络根据loss去学习特征权重，使得有效的feature map权重大，无效或效果小的feature map权重小的方式训练模型达到更好的结果

- 通过在原始网络结构的building block 单元中嵌入SE模块，我们可以获得不同种类的SENet 。如SE-BN-Inception、SE-ResNet 、SE-ReNeXt、SE-Inception-ResNet-v2等等。

### Squeeze-Excitation结构是怎么实现的？
![resnetv2.png](https://github.com/vivienzou1/Deep-Learning-Interview-Book/blob/master/docs/imgs/00081.png)
![resnetv2.png](https://github.com/vivienzou1/Deep-Learning-Interview-Book/blob/master/docs/imgs/00082.png)
**Squeeze**
- 我们顺着空间维度来进行特征压缩，将每个二维的特征通道变成一个实数，这个实数某种程度上具有全局的感受野，并且输出的维度和输入的特征通道数相匹配。它表征着在特征通道上响应的全局分布，而且使得靠近输入的层也可以获得全局的感受野，这一点在很多任务中都是非常有用的。

**Excitation**
- 它是一个类似于循环神经网络中门的机制。通过参数 来为每个特征通道生成权重，其中参数 被学习用来显式地建模特征通道间的相关性。

**Reweight**
- 我们将Excitation的输出的权重看做是进过特征选择后的每个特征通道的重要性，然后通过乘法逐通道加权到先前的特征上，完成在通道维度上的对原始特征的重标定。


## FCN

一句话概括就是：FCN将传统网络后面的全连接层换成了卷积层，这样网络输出不再是类别而是 heatmap；同时为了解决因为卷积和池化对图像尺寸的影响，提出使用上采样的方式恢复。

作者的FCN主要使用了三种技术：

- 卷积化（Convolutional）

- 上采样（Upsample）
- 跳跃结构（Skip Layer）

卷积化

卷积化即是将普通的分类网络，比如VGG16，ResNet50/101等网络丢弃全连接层，换上对应的卷积层即可。

上采样

此处的上采样即是反卷积（Deconvolution）。当然关于这个名字不同框架不同，Caffe和Kera里叫Deconvolution，而tensorflow里叫conv_transpose。CS231n这门课中说，叫conv_transpose更为合适。

众所诸知，普通的池化（为什么这儿是普通的池化请看后文）会缩小图片的尺寸，比如VGG16 五次池化后图片被缩小了32倍。为了得到和原图等大的分割图，我们需要上采样/反卷积。

反卷积和卷积类似，都是相乘相加的运算。只不过后者是多对一，前者是一对多。而反卷积的前向和后向传播，只用颠倒卷积的前后向传播即可。所以无论优化还是后向传播算法都是没有问题。

跳跃结构（Skip Layers）

（这个奇怪的名字是我翻译的，好像一般叫忽略连接结构）这个结构的作用就在于优化结果，因为如果将全卷积之后的结果直接上采样得到的结果是很粗糙的，所以作者将不同池化层的结果进行上采样之后来优化输出。

上采样获得与输入一样的尺寸
文章采用的网络经过5次卷积+池化后，图像尺寸依次缩小了 2、4、8、16、32倍，对最后一层做32倍上采样，就可以得到与原图一样的大小

作者发现，仅对第5层做32倍反卷积（deconvolution），得到的结果不太精确。于是将第 4 层和第 3 层的输出也依次反卷积（图５）

**参考资料**

[【总结】图像语义分割之FCN和CRF](https://zhuanlan.zhihu.com/p/22308032)

[图像语义分割（1）- FCN](https://blog.csdn.net/zizi7/article/details/77093447)

[全卷积网络 FCN 详解](https://www.cnblogs.com/gujianhan/p/6030639.html)

## U-Net

本文介绍一种编码器-解码器结构。编码器逐渐减少池化层的空间维度，解码器逐步修复物体的细节和空间维度。编码器和解码器之间通常存在快捷连接，因此能帮助解码器更好地修复目标的细节。U-Net 是这种方法中最常用的结构。

fcn(fully convolutional natwork)的思想是：修改一个普通的逐层收缩的网络，用上采样(up sampling)(？？反卷积)操作代替网络后部的池化(pooling)操作。因此，这些层增加了输出的分辨率。为了使用局部的信息，在网络收缩过程（路径）中产生的高分辨率特征(high resolution features) ，被连接到了修改后网络的上采样的结果上。在此之后，一个卷积层基于这些信息综合得到更精确的结果。

与fcn(fully convolutional natwork)不同的是，我们的网络在上采样部分依然有大量的特征通道(feature channels)，这使得网络可以将环境信息向更高的分辨率层(higher resolution layers)传播。结果是，扩张路径基本对称于收缩路径。网络不存在任何全连接层(fully connected layers)，并且，只使用每个卷积的有效部分，例如，分割图(segmentation map)只包含这样一些像素点，这些像素点的完整上下文都出现在输入图像中。为了预测图像边界区域的像素点，我们采用镜像图像的方式补全缺失的环境像素。这个tiling方法在使用网络分割大图像时是非常有用的，因为如果不这么做，GPU显存会限制图像分辨率。
我们的训练数据太少，因此我们采用弹性形变的方式增加数据。这可以让模型学习得到形变不变性。这对医学图像分割是非常重要的，因为组织的形变是非常常见的情况，并且计算机可以很有效的模拟真实的形变。在[3]中指出了在无监督特征学习中，增加数据以获取不变性的重要性。

**参考资料**

- [U-net翻译](https://blog.csdn.net/natsuka/article/details/78565229)

## DeepLab 系列

![resnetv2.png](https://github.com/vivienzou1/Deep-Learning-Interview-Book/blob/master/docs/imgs/000DL0-5.png)
![resnetv2.png](https://github.com/vivienzou1/Deep-Learning-Interview-Book/blob/master/docs/imgs/000DL0-4.png)
![resnetv2.png](https://github.com/vivienzou1/Deep-Learning-Interview-Book/blob/master/docs/imgs/000DL0-3.png)
![resnetv2.png](https://github.com/vivienzou1/Deep-Learning-Interview-Book/blob/master/docs/imgs/000DL0-2.png)
![resnetv2.png](https://github.com/vivienzou1/Deep-Learning-Interview-Book/blob/master/docs/imgs/000DL0-1.png)

### DeepLab v1

![resnetv2.png](https://github.com/vivienzou1/Deep-Learning-Interview-Book/blob/master/docs/imgs/000DL1-1.png)
![resnetv2.png](https://github.com/vivienzou1/Deep-Learning-Interview-Book/blob/master/docs/imgs/000DL12.png)

### DeepLab v2

![resnetv2.png](https://github.com/vivienzou1/Deep-Learning-Interview-Book/blob/master/docs/imgs/000DL2-1.png)
![resnetv2.png](https://github.com/vivienzou1/Deep-Learning-Interview-Book/blob/master/docs/imgs/000DL2-2.png)
![resnetv2.png](https://github.com/vivienzou1/Deep-Learning-Interview-Book/blob/master/docs/imgs/000DL2-3.png)

### DeepLab v3

![resnetv2.png](https://github.com/vivienzou1/Deep-Learning-Interview-Book/blob/master/docs/imgs/000DL3-1.png)
![resnetv2.png](https://github.com/vivienzou1/Deep-Learning-Interview-Book/blob/master/docs/imgs/000DL3-2.png)
![resnetv2.png](https://github.com/vivienzou1/Deep-Learning-Interview-Book/blob/master/docs/imgs/000DL3-3.png)
![resnetv2.png](https://github.com/vivienzou1/Deep-Learning-Interview-Book/blob/master/docs/imgs/000DL3-4.png)

**参考资料**

- [Semantic Segmentation --DeepLab(1,2,3)系列总结](https://blog.csdn.net/u011974639/article/details/79148719)

## 边框回顾（Bounding-Box Regression）

如下图所示，绿色的框表示真实值Ground Truth, 红色的框为Selective Search提取的候选区域/框Region Proposal。那么即便红色的框被分类器识别为飞机，但是由于红色的框定位不准(IoU<0.5)， 这张图也相当于没有正确的检测出飞机。

![resnetv2.png](https://github.com/vivienzou1/Deep-Learning-Interview-Book/blob/master/docs/imgs/00085.png)

如果我们能对红色的框进行微调fine-tuning，使得经过微调后的窗口跟Ground Truth 更接近， 这样岂不是定位会更准确。 而Bounding-box regression 就是用来微调这个窗口的。

边框回归是什么？

对于窗口一般使用四维向量(x,y,w,h)(x,y,w,h) 来表示， 分别表示窗口的中心点坐标和宽高。 对于图2, 红色的框 P 代表原始的Proposal, 绿色的框 G 代表目标的 Ground Truth， 我们的目标是寻找一种关系使得输入原始的窗口 P 经过映射得到一个跟真实窗口 G 更接近的回归窗口G^。

![resnetv2.png](https://github.com/vivienzou1/Deep-Learning-Interview-Book/blob/master/docs/imgs/00084.png)

所以，边框回归的目的即是：给定(Px,Py,Pw,Ph)寻找一种映射f， 使得f(Px,Py,Pw,Ph)=(Gx^,Gy^,Gw^,Gh^)并且(Gx^,Gy^,Gw^,Gh^)≈(Gx,Gy,Gw,Gh)

边框回归怎么做的？

![resnetv2.png](https://github.com/vivienzou1/Deep-Learning-Interview-Book/blob/master/docs/imgs/00086.png)

Input:
RegionProposal→P=(Px,Py,Pw,Ph)这个是什么？ 输入就是这四个数值吗？其实真正的输入是这个窗口对应的 CNN 特征，也就是 R-CNN 中的 Pool5 feature（特征向量）。 (注：训练阶段输入还包括 Ground Truth， 也就是下边提到的t∗=(tx,ty,tw,th))

Output:
需要进行的平移变换和尺度缩放 dx(P),dy(P),dw(P),dh(P)，或者说是Δx,Δy,Sw,Sh。我们的最终输出不应该是 Ground Truth 吗？ 是的， 但是有了这四个变换我们就可以直接得到 Ground Truth。

这里还有个问题， 根据(1)~(4)我们可以知道， P 经过 dx(P),dy(P),dw(P),dh(P)得到的并不是真实值 G，而是预测值G^。的确，这四个值应该是经过 Ground Truth 和 Proposal 计算得到的真正需要的平移量(tx,ty)和尺度缩放(tw,th)。 

这也就是 R-CNN 中的(6)~(9)： 
tx=(Gx−Px)/Pw,(6)

ty=(Gy−Py)/Ph,(7)

tw=log(Gw/Pw),(8)

th=log(Gh/Ph),(9)

**参考资料**

- [bounding box regression](http://caffecn.cn/?/question/160)
- [边框回归(Bounding Box Regression)详解](https://blog.csdn.net/zijin0802034/article/details/77685438)

- [什么是边框回归Bounding-Box regression，以及为什么要做、怎么做](https://www.julyedu.com/question/big/kp_id/26/ques_id/2139)

## Pooling层原理
see notebook 2 pg. 10

* 保留主要的特征同时减少参数(降维，效果类似PCA)和计算量，防止过拟合，提高模型泛化能力
* invariance(不变性)，这种不变性包括translation(平移)，rotation(旋转)，scale(尺度)

1. translation invariance: 平移不变性的特点。图中的a（或b）表示，在原始图片中的这些a（或b）位置，最终都会映射到相同的位置。
2. rotation invariance: 第一张相对于x轴有倾斜角，第二张是平行于x轴，两张图片相当于做了旋转，经过多次max pooling后具有相同的特征
![resnetv2.png](https://github.com/vivienzou1/Deep-Learning-Interview-Book/blob/master/docs/imgs/00087.png)
3. scale invariance: 第一张的“0”比较大，第二张的“0”进行了较小，相当于作了缩放，同样地，经过多次max pooling后具有相同的特征

## depthwise卷积

可用来提取特征，但相比于常规卷积操作，其参数量和运算成本较低。所以在一些轻量级网络中会碰到这种结构如MobileNet。

Depthwise Convolution的一个卷积核负责一个通道，一个通道只被一个卷积核卷积。上面所提到的常规卷积每个卷积核是同时操作输入图片的每个通道。
同样是对于一张5×5像素、三通道彩色输入图片（shape为5×5×3），Depthwise Convolution首先经过第一次卷积运算，不同于上面的常规卷积，DW完全是在二维平面内进行。卷积核的数量与上一层的通道数相同（通道和卷积核一一对应）。所以一个三通道的图像经过运算后生成了3个Feature map(如果有same padding则尺寸与输入层相同为5×5)，如下图所示。

Depthwise Convolution完成后的Feature map数量与输入层的通道数相同，无法扩展Feature map。而且这种运算对输入层的每个通道独立进行卷积运算，没有有效的利用不同通道在相同空间位置上的feature信息。因此需要Pointwise Convolution来将这些Feature map进行组合生成新的Feature map

![resnetv2.png](https://github.com/vivienzou1/Deep-Learning-Interview-Book/blob/master/docs/imgs/00088.png)


## pointwise卷积

Pointwise Convolution的运算与常规卷积运算非常相似，它的卷积核的尺寸为 1×1×M，M为上一层的通道数。所以这里的卷积运算会将上一步的map在深度方向上进行加权组合，生成新的Feature map。有几个卷积核就有几个输出Feature map。如下图所示。

![resnetv2.png](https://github.com/vivienzou1/Deep-Learning-Interview-Book/blob/master/docs/imgs/00089.png)

## 为什么降采用使用average pooling，而分类使用max pooling

max-pooling的效果更好，虽然max-pooling和average-pooling都对数据做了下采样，但是max-pooling感觉更像是做了特征选择，选出了分类辨识度更好的特征，提供了非线性，根据相关理论，特征提取的误差主要来自两个方面：（1）邻域大小受限造成的估计值方差增大；（2）卷积层参数误差造成估计均值的偏移。

| max pooling | average pooling |
|-------------|-----------------|
| 分类辨识度更好的特征，提供了非线性| 强调对整体特征信息进行一层下采样，|
| |在减少参数维度的贡献上更大一点，信息的完整传递，在一个很大很有代表性的模型中。|
|减少（2）卷积层参数误差造成估计均值的偏移|减少（1）邻域大小受限造成的估计值方差增大|
| 更多的保留纹理信息 | 更多的保留图像的背景信息| 
| 在模型接近分类器的末端使用全局平均池化还可以代替Flatten操作|比如说DenseNet中的模块之间的连接大多采用average-pooling|
| 使输入数据变成一位向量。|在减少维度的同时，更有利信息传递到下一个模块进行特征提取|

## 组卷积（group convolution）

* pro:
group conv的方式能够增加filter之间的对角相关性。而且能够减少训练参数，不容易过拟合，这类似于正则的效果。

![组卷积操作.png](imgs/DLIB-0015.png)

可以看到，图中将输入数据分成了2组（组数为g），需要注意的是，这种分组只是在深度上进行划分，即某几个通道编为一组，这个具体的数量由（C1/g）决定。因为输出数据的改变，相应的，卷积核也需要做出同样的改变。即每组中卷积核的深度也就变成了（C1/g），而卷积核的大小是不需要改变的，此时每组的卷积核的个数就变成了（C2/g）个，而不是原来的C2了。然后用每组的卷积核同它们对应组内的输入数据卷积，得到了输出数据以后，再用concatenate的方式组合起来，最终的输出数据的通道仍旧是C2。也就是说，分组数g决定以后，那么我们将并行的运算g个相同的卷积过程，每个过程里（每组），输入数据为H1×W1×C1/g，卷积核大小为h1×w1×C1/g，一共有C2/g个，输出数据为H2×W2×C2/g。

举个例子：

Group conv本身就极大地减少了参数。比如当输入通道为256，输出通道也为256，kernel size为3×3，不做Group conv参数为256×3×3×256。实施分组卷积时，若group为8，每个group的input channel和output channel均为32，参数为8×32×3×3×32，是原来的八分之一。而Group conv最后每一组输出的feature maps应该是以concatenate的方式组合。 


**参考资料**

- [A Tutorial on Filter Groups (Grouped Convolution)](https://blog.yani.io/filter-group-tutorial/)

- [深度可分离卷积、分组卷积、扩张卷积、转置卷积（反卷积）的理解](https://blog.csdn.net/chaolei3/article/details/79374563)

## 空洞/扩张卷积（Dilated/Atrous Convolution）

Dilated convolution/Atrous convolution可以叫空洞卷积或者扩张卷积。

背景：语义分割中pooling 和 up-sampling layer层。pooling会降低图像尺寸的同时增大感受野，而up-sampling操作扩大图像尺寸，这样虽然恢复了大小，但很多细节被池化操作丢失了。

需求：能不能设计一种新的操作，不通过pooling也能有较大的感受野看到更多的信息呢？

目的：替代pooling和up-sampling运算，既增大感受野又不减小图像大小。

简述：在标准的 convolution map 里注入空洞，以此来增加 reception field。相比原来的正常convolution，dilated convolution 多了一个 hyper-parameter 称之为 dilation rate 指的是kernel的间隔数量(e.g. 正常的 convolution 是 dilatation rate 1)。

![resnetv2.png](https://github.com/vivienzou1/Deep-Learning-Interview-Book/blob/master/docs/imgs/00090.png)


**参考资料**

[Types of convolution](https://www.youtube.com/watch?v=gmr18xg4wTg)

## 转置卷积（Transposed Convolutions）

第一次出现是 Zeiler 在2010年发表的论文 Deconvolutional networks 中。

**转置卷积和反卷积的区别**

转置卷积只能还原shape大小，而不能还原value。你可以理解成，至少在数值方面上，转置卷积不能实现卷积操作的逆过程。所以说转置卷积与真正的反卷积有点相似，因为两者产生了相同的空间分辨率。但是又名反卷积（deconvolutions）的这种叫法是不合适的，因为它不符合反卷积的概念。

简单来说，转置矩阵就是一种上采样过程。

正常卷积过程如下，利用3x3的卷积核对4x4的输入进行卷积，输出结果为2x2

![卷积过程](https://github.com/vdumoulin/conv_arithmetic/blob/master/gif/no_padding_no_strides.gif?raw=true)

转置卷积过程如下，利用3x3的卷积核对"做了补0"的2x2输入进行卷积，输出结果为4x4。

![转置卷积](https://github.com/vdumoulin/conv_arithmetic/blob/master/gif/no_padding_no_strides_transposed.gif?raw=true)

上述的卷积运算和转置卷积是"尺寸"对应的，卷积的输入大小与转置卷积的输出大小一致，分别可以看成下采样和上采样操作。

**参考资料**

- [Transposed Convolution, Fractionally Strided Convolution or Deconvolution](https://buptldy.github.io/2016/10/29/2016-10-29-deconv/)
- [深度学习 | 反卷积/转置卷积 的理解 transposed conv/deconv](https://blog.csdn.net/u014722627/article/details/60574260)
- [反卷积(Deconvolution)、上采样(UNSampling)与上池化(UnPooling)](https://blog.csdn.net/a_a_ron/article/details/79181108)
- [Transposed Convolution, Fractionally Strided Convolution or Deconvolution](https://buptldy.github.io/2016/10/29/2016-10-29-deconv/)
## 转置卷积（Transposed Convolutions）计算

![resnetv2.png](https://github.com/vivienzou1/Deep-Learning-Interview-Book/blob/master/docs/imgs/00091.png)
![resnetv2.png](https://github.com/vivienzou1/Deep-Learning-Interview-Book/blob/master/docs/imgs/00092.png)
![resnetv2.png](https://github.com/vivienzou1/Deep-Learning-Interview-Book/blob/master/docs/imgs/00093.png)
![resnetv2.png](https://github.com/vivienzou1/Deep-Learning-Interview-Book/blob/master/docs/imgs/00094.png)
![resnetv2.png](https://github.com/vivienzou1/Deep-Learning-Interview-Book/blob/master/docs/imgs/00095.png)
![resnetv2.png](https://github.com/vivienzou1/Deep-Learning-Interview-Book/blob/master/docs/imgs/00096.png)

## Group Normalization

可以替代BN
BN归一化方式对batch是independent的，过小的batch size会导致其性能下降，一般来说每GPU上batch设为32最合适，但是对于一些其他深度学习任务batch size往往只有1-2，比如目标检测，图像分割，视频分类上，输入的图像数据很大，较大的batchsize显存吃不消。

![resnetv2.png](https://github.com/vivienzou1/Deep-Learning-Interview-Book/blob/master/docs/imgs/00097.png)

而GN介于LN和IN之间，其首先将channel分为许多组（group），对每一组做归一化，及先将feature的维度由[N, C, H, W]reshape为[N, G，C//G , H, W]，归一化的维度为[C//G , H, W]

在同一张图像上学习到的特征应该是具有相同的分布，那么，具有相同的特征可以被分到同一个group中，按照个人理解，每一层有很多的卷积核，这些核学习到的特征并不完全是独立的，某些特征具有相同的分布，因此可以被group。导致分组（group）的因素有很多，比如频率、形状、亮度和纹理等，HOG特征根据orientation分组，而对神经网络来讲，其提取特征的机制更加复杂，也更加难以描述，变得不那么直观。另在神经科学领域，一种被广泛接受的计算模型是对cell的响应做归一化，此现象存在于浅层视觉皮层和整个视觉系统。

## Xception
![resnetv2.png](https://github.com/vivienzou1/Deep-Learning-Interview-Book/blob/master/docs/imgs/00098.png)
![resnetv2.png](https://github.com/vivienzou1/Deep-Learning-Interview-Book/blob/master/docs/imgs/00099.png)
![resnetv2.png](https://github.com/vivienzou1/Deep-Learning-Interview-Book/blob/master/docs/imgs/00100.png)

* pro:
1. 比于Inception V3更多的准确率提升
2. 和Inception V3相比，Xception的参数量有所下降
3. 收敛过程也比Inception V3更快

## SKNet

* pro:
- update version of SENet with higher accuracy

* con:
- little bit slower than SENet

![resnetv2.png](https://github.com/vivienzou1/Deep-Learning-Interview-Book/blob/master/docs/imgs/00101.png)
![resnetv2.png](https://github.com/vivienzou1/Deep-Learning-Interview-Book/blob/master/docs/imgs/00102.png)

* 很多网络使用了各种Trick来降低计算量，比如ResNeXt，计算量大大减少，精度却略有提升。那么如果不牺牲那么多计算量，能否精度提高一些呢？比如使用大一点的Kernel，如 5✖️5 的卷积提升精度。
* 结合现在普遍使用的Attention操作
* 加了上面两个操作之后，显然计算量会上去，于是作者再加了一个Group Convlution来做trade off。

**参考资料**

- [SKNet——SENet孪生兄弟篇](https://zhuanlan.zhihu.com/p/59690223)
- [后ResNet时代：SENet与SKNet](https://zhuanlan.zhihu.com/p/60187262)

## GCNet
* concept:

1. 通过可视化Non-local中不同position的attention map来发现其almost一样，故而设计了position-independent 的简化版non-local网络（SNL）
2. 通过几个Ablation实验说明（下图是其中一个在ImageNet测的），non-local based pooling 方式性能与 vanilla average pooling 基本差不多（稍好一点），而对于GCNet 或者SENet来说，前者比后者性能有显著提高的操作源于 how global context is aggregated to query positions：SENet中是 sigmoid 然后 boardcast 相乘；而GCNet 中是直接 boardcast 相加。

![resnetv2.png](https://github.com/vivienzou1/Deep-Learning-Interview-Book/blob/master/docs/imgs/00103.png)

从上图可见:

一、(b) SNL中的 Context modeling 采用的是 position-share but channel-independent的 global attention pooling，故输出tensor维度为 Cx1x1 （而老版本Non-local这里输出的是 CxHxW）。

二、 (d)GC block就是在 (b)SNL 中嵌入了 SE block（FC-LN-ReLU-FC），来减少参数同时增强channel间的信息交互。

**参考资料**

[模块设计之 SKNet, GCNet, GloRe, Octave](https://www.tensorinfinity.com/paper_157.html)

## Octave Convolution

![resnetv2.png](https://github.com/vivienzou1/Deep-Learning-Interview-Book/blob/master/docs/imgs/00104.png)
![resnetv2.png](https://github.com/vivienzou1/Deep-Learning-Interview-Book/blob/master/docs/imgs/00105.png)

**参考资料**

- [如何评价最新的Octave Convolution？](https://www.zhihu.com/question/320462422/)

## MobileNet 系列（V1-V3）

追求效果与准确性的平衡，给轻量级网络设计

### MobileNetV1

* concept:
- Depthwise Seperable Convolution: depthwise conv + pointwise conv

* pro:
1. much smaller computation compelxity and faster
2. less parameter

* con:
1. light duty
2. 结构其实非常navie
3. Depthwise 部分的kernel比较容易训废掉： 训完之后发现depthwise训出来的kernel有不少是空的...    当时我们认为是因为depthwise每个kernel dim 相对于vanilla conv要小得多， 过小的kernel_dim， 加上ReLU的激活影响下， 使得神经元输出很容易变为0， 所以就学废了： ReLU对于0的输出的梯度为0， 所以一旦陷入了0输出， 就没法恢复了。

![resnetv2.png](https://github.com/vivienzou1/Deep-Learning-Interview-Book/blob/master/docs/imgs/00106.png)
![resnetv2.png](https://github.com/vivienzou1/Deep-Learning-Interview-Book/blob/master/docs/imgs/00107.png)
![resnetv2.png](https://github.com/vivienzou1/Deep-Learning-Interview-Book/blob/master/docs/imgs/00108.png)
![resnetv2.png](https://github.com/vivienzou1/Deep-Learning-Interview-Book/blob/master/docs/imgs/00109.png)
![resnetv2.png](https://github.com/vivienzou1/Deep-Learning-Interview-Book/blob/master/docs/imgs/00110.png)

**参考资料**

- [深度解读谷歌MobileNet](https://blog.csdn.net/t800ghb/article/details/78879612)

### MobileNetV2

* concept:
倒置残差块 和 线性瓶颈层

具有线性bottleneck的倒置残差层。该模块以一个低维的压缩特征作为输入，首先被扩展到高维，然后用一个轻量级的深度卷积过滤。特征随后用线性卷积投影回低维表示。

* pro:
1. suit-able for mobile designs
2. significantly reduce the memory
3. provide smallamounts of very fast software controlled cache memory.

Linear Bottleneck 通过去掉Eltwise+ 的特征去掉ReLU， 减少ReLU对特征的破坏； Invered residual 有两个好处： 1. 复用特征， 2. 旁支block内先通过1x1升维， 再接depthwise conv以及ReLU,  通过增加ReLU的InputDim， 来缓解特征的退化情况.

由此看Inverted residual 确实是个非常精妙的设计！ 其实业界在做优化加速， 确实是把好几个层一起做了， 利用时空局部性，减少访问DDR来加速。 所以 Inverted residual 带宽上确实是比较友好。

![resnetv2.png](https://github.com/vivienzou1/Deep-Learning-Interview-Book/blob/master/docs/imgs/00111.png)
![resnetv2.png](https://github.com/vivienzou1/Deep-Learning-Interview-Book/blob/master/docs/imgs/00112.png)

### MobileNetV3
* pro:
1. more accurate than v2
2. comparable latency
3. faster than v2

* concept:
网络架构搜索

- [如何评价google Searching for MobileNetV3？](https://www.zhihu.com/question/323419310)

![resnetv2.png](https://github.com/vivienzou1/Deep-Learning-Interview-Book/blob/master/docs/imgs/00113.png)
![resnetv2.png](https://github.com/vivienzou1/Deep-Learning-Interview-Book/blob/master/docs/imgs/00114.png)

### MobileNet系列为什么快？各有多少层？多少参数？

depth separable convolution: depthwise conv + pointwise conv

### MobileNetV1、MobileNetV2和MobileNetV3有什么区别

MobileNetv1：在depthwise separable convolutions（参考Xception）方法的基础上提供了高校模型设计的两个选择：宽度因子（width multiplie）和分辨率因子（resolution multiplier）。深度可分离卷积depthwise separable convolutions（参考Xception）的本质是冗余信息更小的稀疏化表达。

下面介绍两幅Xception中 depthwise separable convolution的图示：

![image.png](imgs/DLIB-0018.png)

![image.png](imgs/DLIB-0019.png)

深度可分离卷积的过程是①用16个3×3大小的卷积核（1通道）分别与输入的16通道的数据做卷积（这里使用了16个1通道的卷积核，输入数据的每个通道用1个3×3的卷积核卷积），得到了16个通道的特征图，我们说该步操作是depthwise（逐层）的，在叠加16个特征图之前，②接着用32个1×1大小的卷积核（16通道）在这16个特征图进行卷积运算，将16个通道的信息进行融合（用1×1的卷积进行不同通道间的信息融合），我们说该步操作是pointwise（逐像素）的。这样我们可以算出整个过程使用了3×3×16+（1×1×16）×32 =656个参数。

注：上述描述与标准的卷积非常的不同，第一点在于使用非1x1卷积核时，是单channel的（可以说是1通道），即上一层输出的每个channel都有与之对应的卷积核。而标准的卷积过程，卷积核是多channel的。第二点在于使用1x1卷积核实现多channel的融合，并利用多个1x1卷积核生成多channel。表达的可能不是很清楚，但结合图示其实就容易明白了。

一般卷积核的channel也常称为深度（depth），所以叫做深度可分离，即原来为多channel组合，现在变成了单channel分离。

**参考资料**

- [深度解读谷歌MobileNet](https://blog.csdn.net/t800ghb/article/details/78879612)
- [对深度可分离卷积、分组卷积、扩张卷积、转置卷积（反卷积）的理解](https://blog.csdn.net/chaolei3/article/details/79374563)

### MobileNetv2为什么会加shortcut？

当我们在上面的block中加入短路连接，它就变成了残差结构，但是这个shortcut connection是在bottlenecks之间。这与常规的残差block是相反的，可以看到两者的bottleneck的位置恰恰相反，paper里面称这种相反的残差block为inverted residual block。前面也说了，大家可能会疑惑为什么要采用这种相反的结构呢，paper说这种结果可以在实现上减少内存的使用，

### MobileNet V2中的Residual结构最先是哪个网络提出来的？

SENet

## ShuffleNet 系列（V1-V2++）
light duty

### ShuffleNetV1
Group convolution和Channel shuffle改进ResNet，可以看作是ResNet的压缩版本

![resnetv2.png](https://github.com/vivienzou1/Deep-Learning-Interview-Book/blob/master/docs/imgs/000SN1-1.png)

假设输入feature为 H✖️W✖️C ， 所有的 1✖️1 卷积数为C ， 3✖️3 Depthwise卷积数为 k ，Group convolution都分为 g 组。
图14(b) ShuffleNet stride=1结构： `HW(2Ck/g+9k)+(shuffle cost)`

ShuffleNet的本质是将卷积运算限制在每个Group内，这样模型的计算量取得了显著的下降。然而导致模型的信息流限制在各个Group内，组与组之间没有信息交换，如图15，这会影响模型的表示能力。因此，需要引入组间信息交换的机制，即Channel Shuffle操作。同时Channel Shuffle是可导的，可以实现end-to-end一次性训练网络。

![resnetv2.png](https://github.com/vivienzou1/Deep-Learning-Interview-Book/blob/master/docs/imgs/000SN1-2.png)

* con:
1. Shuffle channel在实现的时候需要大量的指针跳转和Memory set，这本身就是极其耗时的；同时又特别依赖实现细节，导致实际运行速度不会那么理想。
2. Shuffle channel规则是人工设计出来的，不是网络自己学出来的。这不符合网络通过负反馈自动学习特征的基本原则，又陷入人工设计特征的老路（如sift/HOG等）。

- [轻量级网络--ShuffleNet论文解读](https://blog.csdn.net/u011974639/article/details/79200559)
- [轻量级网络ShuffleNet v1](https://www.jianshu.com/p/29f4ec483b96)
- [CNN模型之ShuffleNet](https://zhuanlan.zhihu.com/p/32304419)

### ShuffleNetV2
![resnetv2.png](https://github.com/vivienzou1/Deep-Learning-Interview-Book/blob/master/docs/imgs/000SN2.png)

* concept
1. 整体上继续使用Mobilenet V1的Separable convolution降低卷积运算量。
2. feature reuse
3. 采用Inverted residual block结构。该结构使用Point wise convolution先对feature map进行升维，再在升维后的特征接ReLU，减少ReLU对特征的破坏。

**参考资料**

- [](https://zhuanlan.zhihu.com/p/35405071)
- [ShuffleNetV2：轻量级CNN网络中的桂冠](https://zhuanlan.zhihu.com/p/48261931)
- [轻量级神经网络“巡礼”（一）—— ShuffleNetV2](https://zhuanlan.zhihu.com/p/67009992)
- [ShufflenetV2_高效网络的4条实用准则](https://zhuanlan.zhihu.com/p/42288448)
- [ShuffNet v1 和 ShuffleNet v2](https://zhuanlan.zhihu.com/p/51566209)


## 卷积的计算方式也逐渐进化

(a) Regular convolution：AlexNet/VGG使用
(b) Separable convolution block：拆分Regular convolution为Depth wise和Point wise
(c) Separable with linear bottleneck：将ResNet bottleneck引入Separable convolution
(d) bottleneck with expansion layer：将bottleneck结构反过来，相当于“两头细中间粗”

## IGC 系列（V1-V3）

### IGC V1

![resnetv2.png](https://github.com/vivienzou1/Deep-Learning-Interview-Book/blob/master/docs/imgs/000IGC1-1.png)
* pro:
1. 在网络参数量和计算复杂度不变的同时，使得网络变得更宽了
2. 最后还补充了两次组卷积次序可以交换，并不影响结果；
3. 可以将3x3卷积分解成3x1和1x3，可以更加提高效率。

### IGC V2
![resnetv2.png](https://github.com/vivienzou1/Deep-Learning-Interview-Book/blob/master/docs/imgs/000IGC2-1.png)
![resnetv2.png](https://github.com/vivienzou1/Deep-Learning-Interview-Book/blob/master/docs/imgs/000IGC2-2.png)
![resnetv2.png](https://github.com/vivienzou1/Deep-Learning-Interview-Book/blob/master/docs/imgs/000IGC2-3.png)
可以看出第二次组卷积的时候，每一组的通道数仍然很多，文中的话就是比较dense，因此想到对于每一组再进行IGC，这样就能进一步提升计算的效率了

### IGC V3
![resnetv2.png](https://github.com/vivienzou1/Deep-Learning-Interview-Book/blob/master/docs/imgs/000IGC3-1.png)
![resnetv2.png](https://github.com/vivienzou1/Deep-Learning-Interview-Book/blob/master/docs/imgs/000IGC3-2.png)
IGCV3的主要创新点就是先用1x1Group Pointwise Covolution升维，再3x3的组卷积，再通过1x1Group Pointwise Covolution的

**参考资料**

- [微软资深研究员详解基于交错组卷积的高效DNN | 公开课笔记](https://mp.weixin.qq.com/s/ZLIL9A3RS0jj8knbXP9uFQ)

## 学习率如何调整

1. learning rate annealing:比较高的学习速率开始然后慢慢地在训练中降低学习速率
- Step Decay: 学习率经过一定数量的训练 epochs 后下降了一定的百分比

![resnetv2.png](https://github.com/vivienzou1/Deep-Learning-Interview-Book/blob/master/docs/imgs/000lr-1.png)
![resnetv2.png](https://github.com/vivienzou1/Deep-Learning-Interview-Book/blob/master/docs/imgs/000lr-2.png)

2. Cyclical Learning Rates:在两个约束值之间变动, lr_min, lr_max
![resnetv2.png](https://github.com/vivienzou1/Deep-Learning-Interview-Book/blob/master/docs/imgs/000lr-3.png)
![resnetv2.png](https://github.com/vivienzou1/Deep-Learning-Interview-Book/blob/master/docs/imgs/000lr-4.png)

3. Stochastic Gradient Descent with Warm Restarts: an aggressive annealing schedule is combined with periodic "restarts" to the original starting learning rate.
![resnetv2.png](https://github.com/vivienzou1/Deep-Learning-Interview-Book/blob/master/docs/imgs/000lr-5.png)
![resnetv2.png](https://github.com/vivienzou1/Deep-Learning-Interview-Book/blob/master/docs/imgs/000lr-6.png)

where η_t is the learning rate at timestep t (incremented each mini batch), η_i_max and η_i_min define the range of desired learning rates, Tcurrent represents the number of epochs since the last restart (this value is calculated at every iteration and thus can take on fractional values), and Ti defines the number of epochs in an cycle. Let's try to break this equation down.

* The authors note that this learning rate schedule can further be adapted to:
1. Lengthen the cycle as training progresses.
2. Decay ηimax and ηimin after each cycle.

-[Setting the learning rate of your neural network.](https://www.jeremyjordan.me/nn-learning-rate/)

## 神经网络的layer number & filter number

1. 表达能力：在参数数目一定时，增加深度会比增加广度的表达能力更强一些，后面的层可以共享前面层的计算结果。
2. 参数数目：假设每一层的各种超参数相同，把某一层的广度增加一倍时，参数数目增加了单层参数数目的两倍；而增加一层的话，参数数目只增加了单层参数数目的一倍。
3. 计算复杂度：在同一个深度增加广度或者插入一层，增加的计算时间是类似的。对于卷积层来说，由于其复杂度和送入该层的图像尺寸有关，因此在越接近输入的地方增加广度，计算复杂度会增大不少。为了减少计算时间，目前大部分网络结构都是从上到下广度依次增加的

- [神经网络的深度和广度分别有怎样的作用？](https://www.zhihu.com/question/53976311)

## 网络压缩与量化

神经网络权重基本上都是浮点型数据，如果将浮点数据进行二值化，可以节省32倍左右的存储空间。
在存储模型时，有些模型中权重或者偏置项为0或者接近0的数量非常大，需要进行稀疏化。

* Deep Compression
![resnetv2.png](https://github.com/vivienzou1/Deep-Learning-Interview-Book/blob/master/docs/imgs/000DP-1.png)
![resnetv2.png](https://github.com/vivienzou1/Deep-Learning-Interview-Book/blob/master/docs/imgs/000DP-2.png)

* shallow networks：顾名思义，就是用更小的网络来模拟大网络的输入和输出；逻辑上，当浅层网络的参数规模和深度网络参数规模相当的情况下，浅层网络能够接近或者达到深层网络的效果；
* compressing pre-trained deep networks：对网络进行剪裁，以及一些不携带有效信息的权重进行过滤；
* designing compact layers：设计更紧凑的网络结构，其实就是减少每层网络的参数；
* 对参数进行离散化：比如定点化就是一种方案，将浮点型数据转变成整型后进行计算，极限情况比如二值化，就是对已经训练好的网络进行二值化，减少前向计算的时间；
* Network binarization：对网络的权重和激活函数进行二值化，并且二值网络参与训练；甚至将input也进行二值化，完全的二值计算。
* 改变网络结构，矩阵分解，权重稀疏化（接近0的权重置0），权重量化（二值，三值，8-bit，聚类量化），1*1小卷积：参数量化之后一般需要对模型进行retrain

**参考资料**

- [网络压缩-量化方法对比](https://blog.csdn.net/shuzfan/article/details/51678499)

## Batch Size

- GPU, better 32
- 哪种做法收敛更快, 不能太小
- 对于二阶优化算法，减小batch换来的收敛速度提升远不如引入大量噪声导致的性能下降，因此在使用二阶优化算法时，往往要采用大batch哦。此时往往batch设置成几千甚至一两万才能发挥出最佳性能。
- GPU对2的幂次的batch可以发挥更佳的性能

**参考资料**

- [怎么选取训练神经网络时的Batch size?](https://www.zhihu.com/question/61607442)

- [谈谈深度学习中的 Batch_Size](https://blog.csdn.net/lien0906/article/details/79166196)

## BN和Dropout在训练和测试时的差别

| | BN | Dropout|
|-|----|--------|
|training| 每一批的训练数据进行归一化，也即用每一批数据的均值和方差|减少神经元对部分上层神经元的依赖|
|        | 当一个模型训练完成之后，它的所有参数都确定了，包括均值和方差，gamma和bata|类似多个不同网络结构的模型集成起来，减少过拟合的风险|
|testing|进行一个样本的预测，就并没有batch的概念|不需要dropout|
|       |全量训练数据的均值和方差，这个可以通过移动平均法求得||
|为什么不用全量训练集的均值和方差|全量训练集的均值和方差容易过拟合||
|                          |每一批数据的均值和方差有差别,不是固定值，增加模型的鲁棒性，减少过拟合||
| note|shuffle + 用一个较大的batch值，否则，一个batch的数据无法较好得代表训练集的分布，会影响模型训练的效果||
|平衡训练和测试时的差异||失活概率为 p|

**参考资料**

- [BN和Dropout在训练和测试时的差别](https://zhuanlan.zhihu.com/p/61725100)

## 深度学习调参有哪些技巧？

- xavier initializer
- visulization
- Visualize Layer Weights
- Smooth
- Loss设计要合理.
- 观察loss胜于观察准确率, 优化目标是loss.
- Learning Rate设置合理   
+ 太大: loss爆炸, 或者nan   
+ 太小: 半天loss没反映(但是, LR需要降低的情况也是这样, 这里可视化网络中间结果, 不是weights, 有效果, 俩者可视化结果是不一样的, 太小的话中间结果有点水波纹或者噪点的样子, 因为filter学习太慢的原因, 试过就会知道很明显)   
+ 需要进一步降低了: loss在当前LR下一路降了下来, 但是半天不再降了.   
+ 如果有个复杂点的任务, 刚开始, 是需要人肉盯着调LR的. 后面熟悉这个任务网络学习的特性后, 可以扔一边跑去了.   
+ 如果上面的Loss设计那块你没法合理, 初始情况下容易爆, 先上一个小LR保证不爆, 等loss降下来了, 再慢慢升LR, 之后当然还会慢慢再降LR, 虽然这很蛋疼.   + LR在可以工作的最大值下往小收一收, 免得ReLU把神经元弄死了. 当然, 我是个心急的人, 总爱设个大点的.
- 对比训练集和验证集的loss


- 预处理: -mean/std zero-center就够了, PCA, 白化什么的都用不上. 我个人观点, 反正CNN能学习encoder, PCA用不用其实关系不大, 大不了网络里面自己学习出来一个.
- shuffle, shuffle, shuffle.
- 网络原理的理解最重要, CNN的conv这块, 你得明白sobel算子的边界检测.
- Dropout, Dropout, Dropout(不仅仅可以防止过拟合, 其实这相当于做人力成本最低的Ensemble, 当然, 训练起来会比没有Dropout的要慢一点, 同时网络参数你最好相应加一点, 对, 这会再慢一点).
- CNN更加适合训练回答是否的问题, 如果任务比较复杂, 考虑先用分类任务训练一个模型再finetune.无脑用ReLU(CV领域).
- 无脑用3x3.
- 无脑用xavier.
- LRN一类的, 其实可以不用. 不行可以再拿来试试看.
- filter数量2^n.
- 多尺度的图片输入(或者网络内部利用多尺度下的结果)有很好的提升效果.
- 第一层的filter, 数量不要太少. 否则根本学不出来(底层特征很重要).
- sgd adam 这些选择上, 看你个人选择. 一般对网络不是决定性的. 反正我无脑用sgd + momentum.
- batch normalization我一直没用, 虽然我知道这个很好, 我不用仅仅是因为我懒. 所以要鼓励使用batch normalization.
- 不要完全相信论文里面的东西. 结构什么的觉得可能有效果, 可以拿去试试. 
- 你有95%概率不会使用超过40层的模型.
- shortcut的联接是有作用的.
- 暴力调参最可取, 毕竟, 自己的生命最重要. 你调完这个模型说不定过两天这模型就扔掉了.
- 机器, 机器, 机器. 
- Google的inception论文, 结构要好好看看.一些传统的方法, 要稍微了解了解. 我自己的程序就用过1x14的手写filter, 写过之后你看看inception里面的1x7, 7x1 就会会心一笑...

- <https://www.zhihu.com/question/25097993>

## 为什么深度学习中的模型基本用3x3和5x5的卷积（奇数），而不是2x2和4x4的卷积（偶数）？

**参考资料**

- <https://www.zhihu.com/question/321773456>

## 深度学习训练中是否有必要使用L1获得稀疏解?

- [ ] TODO

**参考资料**

- <https://www.zhihu.com/question/51822759>

## EfficientNet

- [ ] TODO

**参考资料**

- [如何评价谷歌大脑的EfficientNet？](https://www.zhihu.com/question/326833457)
- [EfficientNet-可能是迄今为止最好的CNN网络](https://zhuanlan.zhihu.com/p/67834114)
- [EfficientNet论文解读](https://zhuanlan.zhihu.com/p/70369784)
- [EfficientNet：调参侠的福音（ICML 2019）](https://zhuanlan.zhihu.com/p/69349360)

## 如何理解归一化（Normalization）对于神经网络（深度学习）的帮助？

BN最早被认为通过降低所谓**Internal Covariate Shift**，这种想法的出处可考至[Understanding the difficulty of training deep feedforward neural networks](https://link.zhihu.com/?target=http%3A//proceedings.mlr.press/v9/glorot10a/glorot10a.pdf)，想必这也是batch norm作者这么设计的初衷。但是这种想法并没有过多实验支持，比如说去年NeurlPS这篇paper作者做了实验，在batch norm之后加上一些随机扰动（non-zero mean and non-unit variance，人为引入covariate shift），发现效果仍然比不加好很多。为什么放在batch norm layer之后而不是之前？因为为了证伪batch norm通过forward pass这一步降低covariate shift来提升网络训练效率的。这样说来故事就变得很有趣了，也就是说我们大概都理解一些BN对BN层之前网络噪音的好处，那么能不能研究一下它对它后面layer的影响？所以这些研究从优化的角度，有如下几种观点。

1. BN通过修改loss function， 可以令loss的和loss的梯度均满足更强的Lipschitzness性质（即函数f满足L-Lipschitz和 ![[公式]](https://www.zhihu.com/equation?tex=%5Cbeta) -smooth，令L和 ![[公式]](https://www.zhihu.com/equation?tex=%5Cbeta) 更小，后者其实等同于f Hessian的eigenvalue小于 ![[公式]](https://www.zhihu.com/equation?tex=%5Cbeta) ，可以作为光滑程度的度量，其实吧我觉得，一般convex optimization里拿这个度量算convergence rate是神器，对于non-convex optimization，不懂鸭，paper里好像也没写的样子），这么做的好处是当步子迈得大的时候，我们可以更自信地告诉自己计算出来的梯度可以更好地近似实际的梯度，因此也不容易让优化掉进小坑里。有意思的地方来了，是不是我在某些地方插入一个1/1000 layer，把梯度的L-Lipschitz变成1/1000L-Lipschitz就能让函数优化的更好了呢？其实不是的，因为单纯除以函数会改变整个优化问题，而BN做了不仅仅rescale这件事情，还让原来近似最优的点在做完变化之后，仍然保留在原来不远的位置。这也就是这篇文章的核心论点，BN做的是问题reparametrization而不是简单的scaling。 [1]
2. BN把优化这件事情分解成了优化参数的方向和长度两个任务，这么做呢可以解耦层与层之间的dependency因此会让curvature结构更易于优化。这篇证了convergence rate，但由于没有认真读，所以感觉没太多资格评价。[2]

归一化手段是否殊途同归？很可能是的，在[1]的3.3作者也尝试了Lp normalization，也得到了和BN差不多的效果。至于Layer norm还是weight norm，可能都可以顺着这个思路进行研究鸭，无论是通过[1]还是[2]，可能今年的paper里就见分晓了，let's see。

1. [How Does Batch Normalization Help Optimization?](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1805.11604) 
2. [Exponential convergence rates for Batch Normalization: The power of length-direction decoupling in non-convex optimization](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1805.10694)

**参考资料**

- [如何理解归一化（Normalization）对于神经网络（深度学习）的帮助？](https://www.zhihu.com/question/326034346/answer/708331566)

## 多标签分类怎么解决？

- [ ] TODO
   

